!pip -q install optuna scikit-learn tqdm seaborn
!pip -q install optuna scikit-learn tqdm

# Importy
import os
import time
import math
import random
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import f1_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import optuna
from google.colab import drive
from tqdm.notebook import tqdm

# Stałe
SEED = 42
N_TRIALS = 5
EPOCHS = 20
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# Montowanie Dysku Google
drive.mount('/content/drive')
dataset_path = '/content/drive/MyDrive/dataset'
if not os.path.exists(dataset_path):
    raise FileNotFoundError(f"Folder nie istnieje: {dataset_path}")

# Generatory danych
img_size = (96, 96)
batch_size = 16
num_classes = 3

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)
val_test_datagen = ImageDataGenerator(rescale=1./255)

train_data = train_datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'),
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True
)
val_data = val_test_datagen.flow_from_directory(
    os.path.join(dataset_path, 'validation'),
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False
)
test_data = val_test_datagen.flow_from_directory(
    os.path.join(dataset_path, 'test'),
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False
)

# Podgląd danych
images, labels = next(train_data)
fig, axes = plt.subplots(1, 5, figsize=(15, 5))
for i in range(5):
    axes[i].imshow(images[i])
    axes[i].axis('off')
    axes[i].set_title(f'Label idx: {np.argmax(labels[i])}')
plt.show()

# class_weight
y_train_indices = train_data.classes
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_indices),
    y=y_train_indices
)
class_weight_dict = {i: float(w) for i, w in enumerate(class_weights)}
print("class_weight:", class_weight_dict)

# Model
def build_model(trial):
    n_blocks = trial.suggest_int("n_blocks", 2, 4)
    base_filters = trial.suggest_categorical("base_filters", [32, 48, 64])
    use_bn = trial.suggest_categorical("use_bn", [True, False])
    kernel_size_choice = trial.suggest_categorical("kernel_size_choice", [3, 5])
    kernel_size = (kernel_size_choice, kernel_size_choice)
    dense_units = trial.suggest_categorical("dense_units", [64, 128, 256])
    dropout_rate = trial.suggest_float("dropout", 0.2, 0.6, step=0.1)
    optimizer_name = trial.suggest_categorical("optimizer", ["adam", "rmsprop"])
    lr = trial.suggest_float("lr", 1e-4, 3e-3, log=True)

    model = Sequential()
    model.add(Input(shape=(img_size[0], img_size[1], 3)))
    filters = base_filters
    for _ in range(n_blocks):
        model.add(Conv2D(filters, kernel_size, activation='relu', padding='same'))
        if use_bn:
            model.add(BatchNormalization())
        model.add(MaxPooling2D(pool_size=(2, 2)))
        filters *= 2
    model.add(Flatten())
    model.add(Dense(dense_units, activation='relu'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(num_classes, activation='softmax'))

    opt = Adam(learning_rate=lr) if optimizer_name == "adam" else RMSprop(learning_rate=lr)
    model.compile(optimizer=opt, loss='categorical_crossentropy')
    return model

# Callbacki
class OptunaF1Callback(tf.keras.callbacks.Callback):
    def __init__(self, trial, val_data):
        super().__init__()
        self.trial = trial
        self.val_data = val_data
        self.f1_history = []
    def on_epoch_end(self, epoch, logs=None):
        self.val_data.reset()
        y_true = self.val_data.classes
        y_pred = self.model.predict(self.val_data, verbose=0)
        y_pred_labels = np.argmax(y_pred, axis=1)
        f1 = f1_score(y_true, y_pred_labels, average='macro')
        self.f1_history.append(f1)
        self.trial.report(f1, step=epoch)
        if self.trial.should_prune():
            raise optuna.TrialPruned()

class TQDMProgressBar(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs=None):
        self.epochs = self.params['epochs']
        self.epoch_bar = tqdm(total=self.epochs, desc='Epoki', position=0)
    def on_epoch_begin(self, epoch, logs=None):
        self.batch_bar = tqdm(total=self.params['steps'], desc=f'Epoka {epoch+1}', leave=False, position=1)
    def on_batch_end(self, batch, logs=None):
        self.batch_bar.update(1)
    def on_epoch_end(self, epoch, logs=None):
        self.batch_bar.close()
        self.epoch_bar.update(1)
    def on_train_end(self, logs=None):
        self.epoch_bar.close()



# === 7. Funkcja celu Optuny ===
def objective(trial):
    tf.keras.backend.clear_session()
    model = build_model(trial)

    f1_cb = OptunaF1Callback(trial, val_data)
    tqdm_cb = TQDMProgressBar()
    early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

    model.fit(
        train_data,
        validation_data=val_data,
        epochs=10,
        class_weight=class_weight_dict,
        callbacks=[early, reduce, f1_cb, tqdm_cb],
        verbose=0
    )

    return max(f1_cb.f1_history) if f1_cb.f1_history else 0.0

# Optuna
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=N_TRIALS)

print("Najlepsze trial:")
print("  value (F1_macro):", study.best_value)
print("  params:", study.best_params)

# Trening finalny
best_params = study.best_params
final_model = build_model(optuna.trial.FixedTrial(best_params))
final_f1_cb = OptunaF1Callback(trial=optuna.trial.FixedTrial(best_params), val_data=val_data)
tqdm_cb = TQDMProgressBar()
early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)

history = final_model.fit(
    train_data,
    validation_data=val_data,
    epochs=10,
    class_weight=class_weight_dict,
    callbacks=[early, reduce, final_f1_cb, tqdm_cb],
    verbose=0
)
F
# Ewaluacja na teście (raport i macierz pomyłek)
test_data.reset()
y_true_test = test_data.classes
y_pred_test = final_model.predict(test_data, verbose=0)
y_pred_labels_test = np.argmax(y_pred_test, axis=1)

print("\nClassification report (TEST):")
target_names = [k for k, _ in sorted(test_data.class_indices.items(), key=lambda x: x[1])]
print(classification_report(y_true_test, y_pred_labels_test, target_names=target_names, digits=4))

cm = confusion_matrix(y_true_test, y_pred_labels_test)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names)
plt.xlabel('Predykcja')
plt.ylabel('Rzeczywista')
plt.title('Macierz pomyłek (TEST)')
plt.tight_layout()
plt.show()

# Wykres F1 na walidacji (finalny trening)
plt.figure(figsize=(8,4))
plt.plot(range(1, len(final_f1_cb.f1_history)+1), final_f1_cb.f1_history, marker='o')
plt.xlabel('Epoka')
plt.ylabel('Makro F1 (val)')
plt.title('F1 na walidacji podczas treningu')
plt.grid(True, alpha=0.3)
plt.show()

# Krzywe strat
plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Strata - trening')
plt.plot(history.history['val_loss'], label='Strata - walidacja')
plt.xlabel('Epoka')
plt.ylabel('Strata')
plt.title('Krzywe strat')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

plt.figure(figsize=(10,5))
epochs = range(1, len(history.history['loss']) + 1)  # liczba epok = długość listy strat
plt.plot(epochs, history.history['loss'], label='Strata - trening')
plt.plot(epochs, history.history['val_loss'], label='Strata - walidacja')
plt.xlabel('Epoka')
plt.ylabel('Strata')
plt.title('Krzywe strat')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Porównanie F1 dla klas na zbiorze testowym
f1_per_class = f1_score(y_true_test, y_pred_labels_test, average=None)
plt.figure(figsize=(6,4))
sns.barplot(x=target_names, y=f1_per_class)
plt.ylim(0,1)
plt.ylabel('F1')
plt.title('F1 dla poszczególnych klas (TEST)')
plt.show()

# Przykłady błędnych klasyfikacji
misclassified_idx = np.where(y_true_test != y_pred_labels_test)[0]
plt.figure(figsize=(12,6))
for i, idx in enumerate(misclassified_idx[:6]):
    img_path = test_data.filepaths[idx]
    img = plt.imread(img_path)
    plt.subplot(2,3,i+1)
    plt.imshow(img)
    plt.title(f"Prawidłowa: {target_names[y_true_test[idx]]}\n"
              f"Pred: {target_names[y_pred_labels_test[idx]]}")
    plt.axis('off')
plt.tight_layout()
plt.show()
