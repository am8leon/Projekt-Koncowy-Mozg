!pip -q install optuna mlflow seaborn scikit-learn tensorflow matplotlib opencv-python numpy google-colab tqdm pydantic-settings

import os, math, time, json, random
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Input
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tqdm.keras import TqdmCallback
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report, f1_score, accuracy_score
from sklearn.utils.class_weight import compute_class_weight
import mlflow
import mlflow.tensorflow
import optuna
from pydantic_settings import BaseSettings
from typing import Tuple, Dict
from google.colab import drive
drive.mount('/content/drive')

# Mixed precision
tf.keras.mixed_precision.set_global_policy('mixed_float16')

def set_seeds(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

# Config
class Config(BaseSettings):
    mode: str = "optuna"
    optimize_for: str = "f1_macro"
    n_trials: int = 5
    img_size: Tuple[int, int] = (96, 96)
    batch_size: int = 16
    epochs: int = 10
    seed: int = 42
    max_runtime_seconds: int = 12 * 60 * 60

    primary_dataset_path: str = "/content/drive/MyDrive/dataset"
    fallback_dataset_path: str = "/content/drive/MyDrive/dataset_fallback"
    external_dataset_dir: str = "/content/drive/MyDrive/dataset_ext"
    ext_max_samples: int = 300

    mlflow_experiment: str = "brain_mri"
    mlflow_dir: str = "/content/mlruns"

    out_dir: str = "/content/outputs"
    model_save_path: str = "/content/drive/MyDrive/best_model_optuna.h5"
    requirements_path: str = "/content/drive/MyDrive/requirements_project.txt"

cfg = Config()
os.makedirs(cfg.out_dir, exist_ok=True)
set_seeds(cfg.seed)

# MLflow setup
os.makedirs(cfg.mlflow_dir, exist_ok=True)
mlflow.set_tracking_uri(f"file:{cfg.mlflow_dir}")
mlflow.set_experiment(cfg.mlflow_experiment)
mlflow.tensorflow.autolog(log_models=False)

# Data generators
def _safe_flow(datagen, path, name, cfg: Config):
    if not os.path.exists(path) or not os.listdir(path):
        raise FileNotFoundError(f"Brak danych w katalogu {path} ({name})")
    return datagen.flow_from_directory(
        path,
        target_size=cfg.img_size,
        batch_size=cfg.batch_size,
        class_mode="categorical",
        shuffle=(name == "train"),
        seed=cfg.seed,
    )

def create_generators(cfg: Config, augmentation: bool = True):
    dataset_path = cfg.primary_dataset_path if os.path.exists(cfg.primary_dataset_path) else cfg.fallback_dataset_path
    if augmentation:
        train_datagen = ImageDataGenerator(
            rescale=1.0/255,
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            shear_range=0.1,
            zoom_range=0.1,
            horizontal_flip=True,
        )
    else:
        train_datagen = ImageDataGenerator(rescale=1.0/255)
    base_datagen = ImageDataGenerator(rescale=1.0/255)
    train = _safe_flow(train_datagen, os.path.join(dataset_path, "train"), "train", cfg)
    val   = _safe_flow(base_datagen, os.path.join(dataset_path, "validation"), "validation", cfg)
    test  = _safe_flow(base_datagen, os.path.join(dataset_path, "test"), "test", cfg)
    return train, val, test

train_gen, val_gen, test_gen = create_generators(cfg, augmentation=True)
class_indices = train_gen.class_indices
idx_to_class = {v: k for k, v in class_indices.items()}

# Preview images
images, labels = next(train_gen)
fig, axes = plt.subplots(1, 5, figsize=(15, 5))
for i in range(5):
    axes[i].imshow(images[i])
    axes[i].axis('off')
    axes[i].set_title(f'Label idx: {np.argmax(labels[i])}')
plt.show()

# Class weights
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_gen.classes),
    y=train_gen.classes
)
class_weight_dict = {int(i): float(w) for i, w in enumerate(class_weights)}

# External dataset
USE_EXT_DIR = os.path.exists(cfg.external_dataset_dir)
ext_gen, ext_steps = None, None
if USE_EXT_DIR:
    ext_datagen = ImageDataGenerator(rescale=1.0/255)
    ext_gen = ext_datagen.flow_from_directory(
        cfg.external_dataset_dir,
        target_size=cfg.img_size,
        batch_size=cfg.batch_size,
        class_mode='categorical',
        shuffle=False
    )
    if ext_gen.n > cfg.ext_max_samples:
        ext_steps = math.ceil(cfg.ext_max_samples / cfg.batch_size)

# Model
def build_model_optuna(trial, img_size: Tuple[int, int], n_classes: int):
    n_conv_blocks = trial.suggest_int("n_conv_blocks", 2, 3)
    base_filters = trial.suggest_categorical("base_filters", [16, 32, 64])
    kernel_size = trial.suggest_categorical("kernel_size", [3, 5])
    dense_units = trial.suggest_categorical("dense_units", [64, 128, 256])
    dropout_rate = trial.suggest_float("dropout_rate", 0.2, 0.6)
    lr = trial.suggest_float("lr", 1e-5, 1e-2, log=True)
    optimizer_name = trial.suggest_categorical("optimizer", ["adam", "rmsprop"])
    use_batchnorm = trial.suggest_categorical("batchnorm", [True, False])

    model = Sequential(name="cnn_optuna")
    model.add(Input(shape=(*img_size, 3)))
    for i in range(n_conv_blocks):
        filters = base_filters * (2 ** i)
        model.add(Conv2D(filters, (kernel_size, kernel_size), activation="relu", padding="same"))
        if use_batchnorm:
            model.add(BatchNormalization())
        model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(dense_units, activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(n_classes, activation="softmax", dtype="float32"))
    return model, lr, optimizer_name

def compile_model(model, lr, optimizer_name):
    opt = Adam(learning_rate=lr) if optimizer_name == "adam" else RMSprop(learning_rate=lr)
    model.compile(optimizer=opt, loss="categorical_crossentropy",
                  metrics=[tf.keras.metrics.CategoricalAccuracy(name="accuracy")])
    return model

# Optuna objective
def objective(trial):
    tf.keras.backend.clear_session()
    set_seeds(cfg.seed)
    model, lr, opt_name = build_model_optuna(trial, cfg.img_size, train_gen.num_classes)
    model = compile_model(model, lr, opt_name)
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1),
        TqdmCallback(verbose=1)
    ]
    model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=cfg.epochs,
        class_weight=class_weight_dict,
        verbose=0,
        callbacks=callbacks
    )
    y_true = val_gen.classes
    y_pred = np.argmax(model.predict(val_gen, verbose=0), axis=1)
    if cfg.optimize_for == "accuracy":
        return accuracy_score(y_true, y_pred)
    else:
        return f1_score(y_true, y_pred, average="macro")

# Run Optuna
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=cfg.n_trials)

print("Najlepszy wynik (cel):", study.best_value)
print("Najlepsze parametry:", study.best_params)

# Definicja callbacku do śledzenia F1 na walidacji podczas treningu
class F1HistoryCallback(tf.keras.callbacks.Callback):
    def __init__(self, val_gen):
        super().__init__()
        self.val_gen = val_gen
        self.f1_history = []

    def on_epoch_end(self, epoch, logs=None):
        self.val_gen.reset()
        y_true = self.val_gen.classes
        y_prob = self.model.predict(self.val_gen, verbose=0)
        y_pred = np.argmax(y_prob, axis=1)
        f1 = f1_score(y_true, y_pred, average="macro")
        self.f1_history.append(f1)

# Budowa finalnego modelu na najlepszych parametrach
best_trial = optuna.trial.FixedTrial(study.best_params)
final_model, best_lr, best_opt_name = build_model_optuna(best_trial, cfg.img_size, train_gen.num_classes)
final_model = compile_model(final_model, best_lr, best_opt_name)

# Trening finalny z paskiem postępu (Tqdm) i zapisem historii
f1_cb = F1HistoryCallback(val_gen)
callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1),
    TqdmCallback(verbose=1),
    f1_cb
]

mlflow.start_run(run_name="final_model")
history = final_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=cfg.epochs,
    class_weight=class_weight_dict,
    verbose=0,                      # tqdm przejmuje wyświetlanie
    callbacks=callbacks
)

# Zapis modelu
final_model.save(cfg.model_save_path)
mlflow.log_artifact(cfg.model_save_path)

# Krzywe strat
epochs_range = range(1, len(history.history['loss']) + 1)
plt.figure(figsize=(8,4))
plt.plot(epochs_range, history.history['loss'], label='Strata - trening')
plt.plot(epochs_range, history.history['val_loss'], label='Strata - walidacja')
plt.xlabel('Epoka')
plt.ylabel('Strata')
plt.title('Krzywe strat')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
loss_plot_path = os.path.join(cfg.out_dir, "loss_curves.png")
plt.savefig(loss_plot_path)
plt.show()
mlflow.log_artifact(loss_plot_path, artifact_path="plots")

# Krzywe accuracy (jeśli dostępne)
if 'accuracy' in history.history and 'val_accuracy' in history.history:
    epochs_range = range(1, len(history.history['accuracy']) + 1)
    plt.figure(figsize=(8,4))
    plt.plot(epochs_range, history.history['accuracy'], label='Accuracy - trening')
    plt.plot(epochs_range, history.history['val_accuracy'], label='Accuracy - walidacja')
    plt.xlabel('Epoka')
    plt.ylabel('Accuracy')
    plt.title('Dokładność modelu')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    acc_plot_path = os.path.join(cfg.out_dir, "accuracy_curves.png")
    plt.savefig(acc_plot_path)
    plt.show()
    mlflow.log_artifact(acc_plot_path, artifact_path="plots")

# F1 na walidacji podczas treningu
if len(f1_cb.f1_history) > 0:
    epochs_range = range(1, len(f1_cb.f1_history) + 1)
    plt.figure(figsize=(8,4))
    plt.plot(epochs_range, f1_cb.f1_history, marker='o', label='Makro F1 (walidacja)')
    plt.xlabel('Epoka')
    plt.ylabel('Makro F1')
    plt.title('F1 na walidacji podczas treningu')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    f1_plot_path = os.path.join(cfg.out_dir, "val_f1_history.png")
    plt.savefig(f1_plot_path)
    plt.show()
    mlflow.log_artifact(f1_plot_path, artifact_path="plots")

# Funkcja do ewaluacji i rysowania macierzy pomyłek
def evaluate_and_plot(name, model, generator, steps=None, class_names=None):
    generator.reset()
    if steps is None:
        y_true = generator.classes
        y_prob = model.predict(generator, verbose=0)
    else:
        y_prob_parts, y_true_parts = [], []
        for i, (bx, by) in enumerate(generator):
            y_prob_parts.append(model.predict(bx, verbose=0))
            y_true_parts.append(np.argmax(by, axis=1))
            if (i + 1) >= steps:
                break
        y_prob = np.vstack(y_prob_parts)
        y_true = np.concatenate(y_true_parts)

    y_pred = np.argmax(y_prob, axis=1)

    acc = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="macro", zero_division=0)
    cm = confusion_matrix(y_true, y_pred)

    # Wydruk i logi
    print(f"[{name}] acc={acc:.4f} precision={precision:.4f} recall={recall:.4f} f1={f1:.4f}")
    mlflow.log_metric(f"{name}_acc", float(acc))
    mlflow.log_metric(f"{name}_precision", float(precision))
    mlflow.log_metric(f"{name}_recall", float(recall))
    mlflow.log_metric(f"{name}_f1", float(f1))

    # Macierz pomyłek
    plt.figure(figsize=(6,5))
    labels = class_names if class_names is not None else list(range(cm.shape[0]))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predykcja')
    plt.ylabel('Rzeczywista')
    plt.title(f'Macierz pomyłek ({name})')
    plt.tight_layout()
    cm_path = os.path.join(cfg.out_dir, f"{name}_confusion_matrix.png")
    plt.savefig(cm_path)
    plt.show()
    mlflow.log_artifact(cm_path, artifact_path="plots")

    # Raport klasyfikacji (dla pełnego generatora bez limitu steps)
    try:
        if steps is None and hasattr(generator, "class_indices"):
            target_names = [k for k, _ in sorted(generator.class_indices.items(), key=lambda x: x[1])]
            print("\nRaport klasyfikacji:\n")
            print(classification_report(y_true, y_pred, target_names=target_names, digits=4))
            # F1 per class (dla testu)
            if name == "test":
                f1_per_class = f1_score(y_true, y_pred, average=None, labels=list(range(len(target_names))))
                plt.figure(figsize=(8,4))
                sns.barplot(x=target_names, y=f1_per_class)
                plt.ylim(0,1)
                plt.ylabel('F1')
                plt.xlabel('Klasa')
                plt.title('F1 dla poszczególnych klas (TEST)')
                plt.tight_layout()
                perclass_path = os.path.join(cfg.out_dir, "test_f1_per_class.png")
                plt.savefig(perclass_path)
                plt.show()
                mlflow.log_artifact(perclass_path, artifact_path="plots")

                # Przykłady błędnych klasyfikacji (max 6)
                mis_idx = np.where(y_true != y_pred)[0][:6]
                if len(mis_idx) > 0 and hasattr(generator, "filepaths"):
                    plt.figure(figsize=(12,6))
                    for i, idx in enumerate(mis_idx):
                        img_path = generator.filepaths[idx]
                        img = plt.imread(img_path)
                        plt.subplot(2,3,i+1)
                        plt.imshow(img)
                        plt.title(f"Prawidłowa: {target_names[y_true[idx]]}\nPred: {target_names[y_pred[idx]]}")
                        plt.axis('off')
                    plt.tight_layout()
                    mis_path = os.path.join(cfg.out_dir, "test_misclassified_examples.png")
                    plt.savefig(mis_path)
                    plt.show()
                    mlflow.log_artifact(mis_path, artifact_path="plots")
    except Exception as e:
        print(f"[{name}] Ostrzeżenie przy generowaniu raportu: {e}")

    return {"acc": acc, "precision": precision, "recall": recall, "f1": f1, "cm": cm}

# Ewaluacja: walidacja i test
val_metrics = evaluate_and_plot(
    "validation",
    final_model,
    val_gen,
    steps=None,
    class_names=[idx_to_class[i] for i in range(train_gen.num_classes)]
)

test_metrics = evaluate_and_plot(
    "test",
    final_model,
    test_gen,
    steps=None,
    class_names=[idx_to_class[i] for i in range(train_gen.num_classes)]
)

# Ewaluacja na dodatkowym zbiorze (z limitem próbek)
if USE_EXT_DIR and ext_gen is not None:
    ext_class_names = [k for k, _ in sorted(ext_gen.class_indices.items(), key=lambda x: x[1])]
    ext_metrics = evaluate_and_plot(
        "external",
        final_model,
        ext_gen,
        steps=ext_steps,              # ograniczenie próbek, aby zmieścić się w limicie czasu
        class_names=ext_class_names
    )
else:
    print("Brak folderowego zbioru zewnętrznego (dataset_ext). Pomijam ewaluację external.")

# Notatka o ograniczeniach accuracy
note_path = os.path.join(cfg.out_dir, "notes_accuracy.txt")
with open(note_path, "w") as f:
    f.write("Uwaga: accuracy bywa mylące przy niezbalansowanych klasach. Raportuję także makro F1, precision i recall.")
mlflow.log_artifact(note_path, artifact_path="notes")

# requirements.txt
!pip freeze > "{cfg.requirements_path}"
mlflow.log_artifact(cfg.requirements_path, artifact_path="requirements")

mlflow.end_run()

print("\nDONE — trening zakończony. Artefakty, wykresy i metryki zapisane.")
