!pip install \
    "pydantic>=2.0" \
    "pydantic-settings>=2.0" \
    optuna \
    mlflow \
    tqdm \
    keras-tqdm \
    seaborn \
    scikit-learn \
    nibabel \
    pandas \
    chardet \
    tensorflow \
    matplotlib \
    opencv-python \
    numpy \
    google-colab

from google.colab import drive
drive.mount('/content/drive')

# === CONFIG ===
from pydantic_settings import BaseSettings
from typing import Tuple

class Config(BaseSettings):
    mode: str = "optuna"
    optimize_for: str = "accuracy"
    n_trials: int = 5
    img_size: Tuple[int, int] = (96, 96)
    batch_size: int = 8
    epochs: int = 5
    seed: int = 42
    mlflow_experiment: str = "MRI_Optuna_Training"
    model_save_path: str = "/content/drive/MyDrive/best_model_optuna.h5"
    primary_dataset_path: str = "/content/drive/MyDrive/dataset"
    fallback_dataset_path: str = "/content/drive/MyDrive/dataset_fallback"
    req_path: str = "/content/drive/MyDrive/dataset/requirements2.txt"
    best_params_path: str = "/content/drive/MyDrive/best_params_optuna.json"
    brats_validation_paths: dict = {
        "flair":  "/content/drive/MyDrive/brats/BraTS20_Validation_124_flair.nii",
        "t1":     "/content/drive/MyDrive/brats/BraTS20_Validation_124_t1.nii",
        "t1ce":   "/content/drive/MyDrive/brats/BraTS20_Validation_124_t1ce.nii",
        "t2":     "/content/drive/MyDrive/brats/BraTS20_Validation_124_t2.nii"
    }
    max_runtime_seconds: int = 12 * 60 * 60

    class Config:
        env_file = ".env"

# === DATA ===
import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator

def _safe_flow(datagen, path, name, cfg: Config):
    if not os.path.exists(path) or not os.listdir(path):
        raise FileNotFoundError(f"Brak danych w katalogu {path} dla {name}")
    return datagen.flow_from_directory(
        path,
        target_size=cfg.img_size,
        batch_size=cfg.batch_size,
        class_mode="categorical",
        shuffle=(name == "train"),
        seed=cfg.seed,
    )

def create_generators(cfg: Config, augmentation: bool = True):
    dataset_path = cfg.primary_dataset_path if os.path.exists(cfg.primary_dataset_path) else cfg.fallback_dataset_path

    if augmentation:
        train_datagen = ImageDataGenerator(
            rescale=1.0/255,
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            shear_range=0.1,
            zoom_range=0.1,
            horizontal_flip=True,
        )
    else:
        train_datagen = ImageDataGenerator(rescale=1.0/255)

    val_test_datagen = ImageDataGenerator(rescale=1.0/255)

    train = _safe_flow(train_datagen, os.path.join(dataset_path, "train"), "train", cfg)
    val = _safe_flow(val_test_datagen, os.path.join(dataset_path, "validation"), "validation", cfg)
    test = _safe_flow(val_test_datagen, os.path.join(dataset_path, "test"), "test", cfg)
    return train, val, test

# === MODEL ===
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout

def build_model_optuna(trial, img_size: Tuple[int, int], n_classes: int):
    n_conv_blocks = trial.suggest_int("n_conv_blocks", 2, 3)
    base_filters = trial.suggest_categorical("base_filters", [16, 32, 64])
    kernel_size = trial.suggest_categorical("kernel_size", [3, 5])
    dense_units = trial.suggest_categorical("dense_units", [64, 128])
    dropout_rate = trial.suggest_float("dropout_rate", 0.2, 0.6)
    lr = trial.suggest_float("lr", 1e-5, 1e-2, log=True)
    optimizer_name = trial.suggest_categorical("optimizer", ["adam", "rmsprop"])
    use_batchnorm = trial.suggest_categorical("batchnorm", [True, False])

    model = Sequential()
    for i in range(n_conv_blocks):
        filters = base_filters * (2 ** i)
        if i == 0:
            model.add(Conv2D(filters, (kernel_size, kernel_size), activation="relu", padding="same", input_shape=(*img_size, 3)))
        else:
            model.add(Conv2D(filters, (kernel_size, kernel_size), activation="relu", padding="same"))
        if use_batchnorm:
            model.add(BatchNormalization())
        model.add(MaxPooling2D((2, 2)))

    model.add(Flatten())
    model.add(Dense(dense_units, activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(n_classes, activation="softmax"))

    return model, lr, optimizer_name

# === VISUALIZATION ===
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, classification_report, confusion_matrix

def plot_confusion_matrix(cm, class_labels, title="Macierz pomyłek", fname=None):
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(title)
    plt.xlabel("Przewidziane")
    plt.ylabel("Rzeczywiste")
    plt.tight_layout()
    if fname:
        plt.savefig(fname)
    plt.show()
    plt.close()

# === TRAINING ===
import time
import json
import mlflow
import optuna
import numpy as np
import cv2
import nibabel as nib
from tqdm.keras import TqdmCallback
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, Callback
from tensorflow.keras.models import load_model

class TimeLimitCallback(Callback):
    def __init__(self, max_seconds: int):
        super().__init__()
        self.max_seconds = max_seconds
        self.start_time = None

    def on_train_begin(self, logs=None):
        self.start_time = time.time()

    def on_epoch_end(self, epoch, logs=None):
        elapsed = time.time() - self.start_time
        if elapsed > self.max_seconds:
            self.model.stop_training = True
            print(f"\n[TimeLimit] Przerwanie treningu po {elapsed/3600:.2f} h (limit {self.max_seconds/3600:.2f} h).")

class MRITrainer:
    def __init__(self, cfg: Config):
        self.cfg = cfg
        self.train_gen, self.val_gen, self.test_gen = create_generators(cfg)
        self.n_classes = self.train_gen.num_classes
        self.model = None
        mlflow.set_experiment(cfg.mlflow_experiment)

    def _compile(self, model, lr, optimizer_name):
        optimizer = Adam(lr) if optimizer_name == "adam" else RMSprop(lr)
        model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])
        return model

    def _fit_with_progress(self, model, trial=None):
        early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
        time_limit = TimeLimitCallback(self.cfg.max_runtime_seconds - 600)  # bufor 10 min

        history = model.fit(
            self.train_gen,
            validation_data=self.val_gen,
            epochs=self.cfg.epochs,
            callbacks=[TqdmCallback(verbose=1), early_stop, time_limit],
            verbose=1  # pokaż postęp epok
        )

        # Zapis modelu po zakończeniu treningu
        if trial is not None:
            model.save(f"model_trial_{trial.number}.h5")
        else:
            model.save("model_latest.h5")

        # Zwrócenie metryki do Optuny
        val_loss = history.history['val_loss'][-1]
        return val_loss

    def objective(self, trial):
        model, lr, optimizer_name = build_model_optuna(trial, self.cfg.img_size, self.n_classes)
        model = self._compile(model, lr, optimizer_name)

        with mlflow.start_run(nested=True):
            val_loss = self._fit_with_progress(model, trial) 
            y_pred = model.predict(self.val_gen, verbose=0).argmax(axis=1)
            y_true = self.val_gen.classes
            score = accuracy_score(y_true, y_pred) if self.cfg.optimize_for == "accuracy" else f1_score(y_true, y_pred, average="weighted")
            mlflow.log_params(trial.params)
            mlflow.log_metric("val_score", score)
        return score

    def run_optuna(self):
        study = optuna.create_study(direction="maximize")
        study.optimize(self.objective, n_trials=self.cfg.n_trials)

        print("Najlepsze parametry:", study.best_params)
        with open(self.cfg.best_params_path, "w") as f:
            json.dump(study.best_params, f)

        # Finalny model
        best_trial = study.best_trial
        model, lr, optimizer_name = build_model_optuna(best_trial, self.cfg.img_size, self.n_classes)
        model = self._compile(model, lr, optimizer_name)
        self._fit_with_progress(model)
        model.save(self.cfg.model_save_path)
        self.model = model

    def run_default(self):
        # Użyj domyślnych parametrów bez Optuny
        class DummyTrial:
            def suggest_int(self, name, low, high): return low
            def suggest_categorical(self, name, choices): return choices[0]
            def suggest_float(self, name, low, high, log=False): return low
        trial = DummyTrial()
        model, lr, optimizer_name = build_model_optuna(trial, self.cfg.img_size, self.n_classes)
        model = self._compile(model, lr, optimizer_name)
        self._fit_with_progress(model)
        model.save(self.cfg.model_save_path)
        self.model = model

    def evaluate(self):
        if self.model is None:
            self.model = load_model(self.cfg.model_save_path)

        y_true = self.test_gen.classes
        y_pred = self.model.predict(self.test_gen, verbose=0).argmax(axis=1)
        acc = accuracy_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred, average="weighted")
        print(f" Accuracy: {acc:.4f}, F1-score: {f1:.4f}")

        cm = confusion_matrix(y_true, y_pred)
        plot_confusion_matrix(cm, class_labels=list(self.test_gen.class_indices.keys()))

        print("\n Raport klasyfikacji:")
        print(classification_report(y_true, y_pred, target_names=list(self.test_gen.class_indices.keys())))

# === MAIN ===
if __name__ == "__main__":
    cfg = Config()
    trainer = MRITrainer(cfg)

    if cfg.mode == "optuna":
        trainer.run_optuna()
    else:
        trainer.run_default()

    trainer.evaluate()
