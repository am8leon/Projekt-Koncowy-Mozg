!pip install -q mlflow==2.12.1 azureml-mlflow==1.55.0 pandas==2.2.2 chardet==5.2.0 \ uvicorn==0.29.0 fastapi==0.110.0 seaborn==0.13.2 nibabel==5.2.1
import os
import json
import cv2
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import mlflow

from typing import Dict, Tuple, List

from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    accuracy_score,
    roc_curve,
    auc,
)
from sklearn.preprocessing import label_binarize

from tensorflow.keras import Model
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import (
    Conv2D,
    MaxPooling2D,
    BatchNormalization,
    Flatten,
    Dense,
    Dropout,
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Stałe
SEED = 42
np.random.seed(SEED)
import random
random.seed(SEED)
tf.random.set_seed(SEED)

# Przydatne w Colabie — cichsze logi
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"  # 0 = wszystko, 2 = warnings+errors

# Google Drive

try:
    from google.colab import drive  # type: ignore
    if not os.path.ismount("/content/drive"):
        drive.mount("/content/drive")
except Exception:
    pass  # jeśli nie w Colabie

# Stałe i ścieżki
IMG_SIZE: Tuple[int, int] = (96, 96)
BATCH_SIZE: int = 16
EPOCHS: int = 10

PRIMARY_DATASET_PATH = "/content/drive/MyDrive/dataset"
FALLBACK_DATASET_PATH = "/content/drive/MyDrive/dataset_fallback"

DATASET_PATH = (
    PRIMARY_DATASET_PATH
    if os.path.exists(os.path.join(PRIMARY_DATASET_PATH, "train"))
    else FALLBACK_DATASET_PATH
)
if not os.path.exists(DATASET_PATH):
    raise FileNotFoundError(
        f"Nie znaleziono zbioru danych ani w {PRIMARY_DATASET_PATH} ani w {FALLBACK_DATASET_PATH}"
    )

MODEL_SAVE_PATH = "/content/drive/MyDrive/best_model_mri.h5"
MLFLOW_EXPERIMENT = "MRI_Training"
FORCE_TRAIN = False  # ustaw True, aby wymusić ponowny trening

# Pomocnicze I/O
import chardet

def read_csv_with_fallback(filepath: str, encodings: List[str] = None) -> pd.DataFrame:
    encodings = encodings or [
        "utf-8",
        "utf-8-sig",
        "latin1",
        "cp1250",
        "iso-8859-2",
    ]
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Plik nie istnieje: {filepath}")
    with open(filepath, "rb") as f:
        _ = chardet.detect(f.read(10000))
    for enc in encodings:
        try:
            with open(filepath, "r", encoding=enc) as f:
                sample = f.read(1024)
                delimiter = ";" if ";" in sample else ","
            df = pd.read_csv(filepath, encoding=enc, sep=delimiter)
            return df
        except Exception:
            continue
    raise ValueError(f"Nie udało się wczytać pliku {filepath}")

# Generatory danych (bez duplikatów i z walidacją danych)

def _safe_flow(datagen: ImageDataGenerator, path: str, name: str):
    if not os.path.exists(path) or not os.listdir(path):
        raise FileNotFoundError(f"Brak danych w katalogu {path} dla {name}")
    return datagen.flow_from_directory(
        path,
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode="categorical",
        shuffle=(name == "train"),
        seed=SEED,
    )

def create_generators(
    rescale_only: bool = True,
) -> Tuple[tf.keras.utils.Sequence, tf.keras.utils.Sequence, tf.keras.utils.Sequence]:
    if rescale_only:
        train_datagen = ImageDataGenerator(rescale=1.0 / 255)
    else:
        train_datagen = ImageDataGenerator(
            rescale=1.0 / 255,
            rotation_range=20,
            width_shift_range=0.2,
            height_shift_range=0.2,
            shear_range=0.2,
            zoom_range=0.2,
            horizontal_flip=True,
        )
    val_test_datagen = ImageDataGenerator(rescale=1.0 / 255)

    train = _safe_flow(train_datagen, os.path.join(DATASET_PATH, "train"), "train")
    val = _safe_flow(val_test_datagen, os.path.join(DATASET_PATH, "validation"), "validation")
    test = _safe_flow(val_test_datagen, os.path.join(DATASET_PATH, "test"), "test")
    return train, val, test

# Model
def build_model(variant: str = "baseline", lr: float = 1e-3) -> Sequential:
    layers = []
    filters_list = [32, 64, 128]
    for i, filters in enumerate(filters_list):
        if i == 0:
            layers.append(Conv2D(filters, (3, 3), activation="relu", padding="same", input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)))
        else:
            layers.append(Conv2D(filters, (3, 3), activation="relu", padding="same"))
        if "batchnorm" in variant:
            layers.append(BatchNormalization())
        layers.append(MaxPooling2D((2, 2)))
    layers += [
        Flatten(),
        Dense(128, activation="relu"),
    ]
    if "dropout" in variant:
        layers.append(Dropout(0.5))
    layers.append(Dense(3, activation="softmax"))

    model = Sequential(layers)
    model.compile(optimizer=Adam(learning_rate=lr), loss="categorical_crossentropy", metrics=["accuracy"])
    return model

# Wykresy i metryki
def plot_confusion_matrix(cm: np.ndarray, class_labels: List[str], title: str = "Macierz pomyłek", fname: str = None):
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(title)
    plt.xlabel("Przewidziane")
    plt.ylabel("Rzeczywiste")
    plt.tight_layout()
    if fname:
        plt.savefig(fname)
    plt.show()
    plt.close()


def plot_error_histogram(errors: np.ndarray):
    plt.figure(figsize=(8, 5))
    sns.histplot(errors.astype(int), bins=2, discrete=True)
    plt.xticks([0, 1], ["Poprawne", "Błędne"])
    plt.title("Histogram błędnych predykcji")
    plt.tight_layout()
    plt.show()
    plt.close()


def plot_predicted_class_distribution(y_pred: np.ndarray, n_classes: int):
    plt.figure(figsize=(8, 5))
    sns.histplot(y_pred, bins=n_classes, discrete=True)
    plt.title("Rozkład przewidywanych klas")
    plt.xlabel("Klasa (indeks)")
    plt.ylabel("Liczba predykcji")
    plt.tight_layout()
    plt.show()
    plt.close()


def plot_roc_auc_ovr(y_true: np.ndarray, y_pred_probs: np.ndarray, class_labels: List[str]):
    n_classes = len(class_labels)
    y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))
    fpr, tpr, roc_auc = {}, {}, {}
    plt.figure(figsize=(8, 6))
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
        plt.plot(fpr[i], tpr[i], label=f"{class_labels[i]} (AUC = {roc_auc[i]:.2f})")
    plt.plot([0, 1], [0, 1], "k--", label="Losowa linia")
    plt.title("ROC Curve dla każdej klasy")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend()
    plt.tight_layout()
    plt.show()
    plt.close()

# Grad-CAM (odporny na Keras 3)
from tensorflow import keras

def find_last_conv_layer(m: Model) -> str:
    # przeszukuje także zagnieżdżone modele
    for layer in reversed(m.layers):
        if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D, keras.layers.DepthwiseConv2D)):
            return layer.name
        if isinstance(layer, keras.Model):
            try:
                return find_last_conv_layer(layer)
            except ValueError:
                pass
    raise ValueError("Brak warstwy konwolucyjnej (Conv2D) w modelu.")


def make_gradcam_heatmap(img_array: np.ndarray, model: Model, last_conv_layer_name: str, target_size: Tuple[int, int]) -> np.ndarray:
    # rozruch modelu — definiuje input/output w Keras 3
    _ = model(img_array, training=False)
    last_conv_layer = model.get_layer(last_conv_layer_name)
    grad_model = keras.Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array, training=False)
        class_idx = tf.argmax(predictions[0])
        loss = predictions[:, class_idx]

    grads = tape.gradient(loss, conv_outputs)  # (1, H, W, C)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]

    heatmap = tf.reduce_sum(conv_outputs * pooled_grads, axis=-1)
    heatmap = tf.maximum(heatmap, 0)
    max_val = tf.reduce_max(heatmap)
    heatmap = (heatmap / (max_val + 1e-8)).numpy()
    heatmap = cv2.resize(heatmap, target_size)
    return heatmap


def overlay_heatmap(orig_img_uint8_rgb: np.ndarray, heatmap: np.ndarray, alpha: float = 0.4) -> np.ndarray:
    heatmap_uint8 = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)
    return cv2.addWeighted(orig_img_uint8_rgb, 1 - alpha, heatmap_color, alpha, 0)

# Trening + ewaluacja (jeden spójny przebieg, bez powtórek)
def train_once(train_gen, val_gen, *, variant: str = "baseline", lr: float = 1e-3) -> Model:
    mlflow.set_experiment(MLFLOW_EXPERIMENT)
    start_args = dict(run_name=f"Train_{variant}_lr{lr}")
    if mlflow.active_run() is not None:
        start_args["nested"] = True

    with mlflow.start_run(**start_args):
        model = build_model(variant=variant, lr=lr)
        history = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, verbose=2)

        val_acc = float(np.max(history.history.get("val_accuracy", [0.0])))
        mlflow.log_params({"variant": variant, "learning_rate": lr})
        mlflow.log_metric("val_accuracy", val_acc)

        # zapis modelu
        model.save(MODEL_SAVE_PATH)
        mlflow.log_artifact(MODEL_SAVE_PATH)
        print(f"Model zapisany do: {MODEL_SAVE_PATH}")
        return model


def evaluate_model(model: Model, test_gen) -> Dict:
    y_true = test_gen.classes
    y_pred_probs = model.predict(test_gen, verbose=0)
    y_pred = np.argmax(y_pred_probs, axis=1)

    acc = float(accuracy_score(y_true, y_pred))
    report_txt = classification_report(y_true, y_pred)

    idx_to_class = {v: k for k, v in test_gen.class_indices.items()}
    class_labels = [idx_to_class[i] for i in range(len(idx_to_class))]

    cm = confusion_matrix(y_true, y_pred)
    plot_confusion_matrix(cm, class_labels, fname="confusion_matrix.png")

    # artefakty MLflow
    mlflow.log_metric("test_accuracy", acc)
    with open("classification_report.txt", "w") as f:
        f.write(report_txt)
    mlflow.log_artifact("classification_report.txt")
    mlflow.log_artifact("confusion_matrix.png")

    # histogramy i ROC
    errors = (y_true != y_pred)
    plot_error_histogram(errors)
    plot_predicted_class_distribution(y_pred, n_classes=len(class_labels))
    plot_roc_auc_ovr(y_true, y_pred_probs, class_labels)

    return {
        "test_accuracy": acc,
        "report": report_txt,
        "class_labels": class_labels,
    }


# Grad-CAM – przykład użycia
def run_gradcam_example(model: Model, test_gen):
    last_conv = find_last_conv_layer(model)
    print(f"Ostatnia warstwa konwolucyjna: {last_conv}")

    img_path = test_gen.filepaths[0]
    from tensorflow.keras.utils import load_img, img_to_array
    img = load_img(img_path, target_size=IMG_SIZE)
    img_arr = img_to_array(img) / 255.0
    img_batch = np.expand_dims(img_arr, axis=0)

    # rozruch i mapa
    _ = model(img_batch, training=False)
    heatmap = make_gradcam_heatmap(img_batch, model, last_conv, target_size=IMG_SIZE)
    img_rgb_uint8 = np.uint8(img_arr * 255)
    overlaid = overlay_heatmap(img_rgb_uint8, heatmap)

    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plt.imshow(img_rgb_uint8)
    plt.title("Oryginalny obraz")
    plt.axis("off")
    plt.subplot(1, 2, 2)
    plt.imshow(overlaid)
    plt.title("Grad-CAM")
    plt.axis("off")
    plt.show()

# CSV – przykład wczytania (opcjonalny, jeśli masz pliki)

def try_read_aux_csvs():
    csv_files = [
        "name_mapping_validation_data.csv",
        "name_mapping.csv",
        "survival_evaluation.csv",
        "survival_info.csv",
    ]
    for csv_name in csv_files:
        full_path = os.path.join(DATASET_PATH, csv_name)
        if os.path.exists(full_path):
            _ = read_csv_with_fallback(full_path)
        else:
            print(f"Brak pliku {csv_name} — pomijam")

# MAIN
if __name__ == "__main__":
    print("Przygotowanie generatorów…")
    train_gen, val_gen, test_gen = create_generators(rescale_only=True)

    mlflow.set_experiment(MLFLOW_EXPERIMENT)
    start_args = dict(run_name="Main_Run")
    if mlflow.active_run() is not None:
        start_args["nested"] = True

    with mlflow.start_run(**start_args):
        if os.path.exists(MODEL_SAVE_PATH) and not FORCE_TRAIN:
            print("Wczytuję istniejący model…")
            model = load_model(MODEL_SAVE_PATH)
        else:
            print("Trenuję model (jednorazowo)…")
            model = train_once(train_gen, val_gen, variant="baseline", lr=1e-3)

        print("Ewaluacja na zbiorze testowym…")
        eval_out = evaluate_model(model, test_gen)
        print(f"Test accuracy: {eval_out['test_accuracy']:.4f}")

        print("Przykład Grad-CAM…")
        run_gradcam_example(model, test_gen)

    print("Gotowe.")
