!pip install -q "pydantic>=2.0" "pydantic-settings>=2.0" optuna optuna-integration mlflow tqdm seaborn scikit-learn nibabel pandas chardet tensorflow matplotlib opencv-python numpy

# Importy
import os, math, time, json, random
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback

from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight

import mlflow
import mlflow.tensorflow
import optuna

from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Tuple, Optional, Dict

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

mlflow.set_tracking_uri("file:/content/drive/MyDrive/mlruns")

# ================================
# OPTYMALIZACJA 1: Konfiguracja GPU i Mixed Precision
# ================================
# Konfiguracja GPU dla maksymalnej wydajności
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f" GPU detected: {len(gpus)} device(s)")
    except RuntimeError as e:
        print(e)

# Mixed precision dla przyspieszenia (2-3x szybciej)
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# XLA compilation dla dodatkowego przyspieszenia
tf.config.optimizer.set_jit(True)

# ================================
# Konfiguracja projektu
# ================================
class Config(BaseSettings):
    mode: str = "optuna"
    optimize_for: str = "f1"

    # OPTYMALIZACJA 2: Parametry zoptymalizowane pod 12h
    n_trials: int = 5
    img_size: Tuple[int, int] = (96, 96)
    batch_size: int = 32
    epochs: int = 10
    seed: int = 42
    max_runtime_seconds: int = 24 * 60 * 60  # 12 godzin

    # Dodatkowe zabezpieczenia czasowe
    max_time_per_trial: int = 40 * 60  # 40 minut per trial (zapas bezpieczeństwa)

    # OPTYMALIZACJA 3: Zwiększony batch size dla szybszego przetwarzania
    prefetch_buffer: int = tf.data.AUTOTUNE
    num_parallel_calls: int = tf.data.AUTOTUNE
    cache_data: bool = True

    primary_dataset_path: str = "/content/drive/MyDrive/dataset"
    fallback_dataset_path: str = "/content/drive/MyDrive/dataset_fallback"
    external_dataset_dir: Optional[str] = None
    ext_max_samples: int = 1000

    brats_validation_paths_primary: Dict[str, str] = {
        "flair": "/content/drive/MyDrive/dataset/BraTS20_Validation_124_flair.nii",
        "t1": "/content/drive/MyDrive/dataset/BraTS20_Validation_124_t1.nii",
        "t1ce": "/content/drive/MyDrive/dataset/BraTS20_Validation_124_t1ce.nii",
        "t2": "/content/drive/MyDrive/dataset/BraTS20_Validation_124_t2.nii",
    }
    brats_validation_paths_fallback: Dict[str, str] = {
        "flair": "/content/drive/MyDrive/brats/BraTS20_Validation_124_flair.nii",
        "t1": "/content/drive/MyDrive/brats/BraTS20_Validation_124_t1.nii",
        "t1ce": "/content/drive/MyDrive/brats/BraTS20_Validation_124_t1ce.nii",
        "t2": "/content/drive/MyDrive/brats/BraTS20_Validation_124_t2.nii",
    }

    model_save_path: str = "/content/drive/MyDrive/best_model_optuna.h5"
    best_params_path: str = "/content/drive/MyDrive/best_params_optuna.json"
    req_path: str = "/content/drive/MyDrive/dataset/requirements2.txt"
    mlflow_experiment: str = "brain_mri"
    mlflow_dir: str = "/content/mlruns"

    model_config = SettingsConfigDict(env_file=".env", extra="ignore")

cfg = Config()

def set_seeds(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

set_seeds(cfg.seed)

os.makedirs(cfg.mlflow_dir, exist_ok=True)
mlflow.set_tracking_uri(f"file:{cfg.mlflow_dir}")
mlflow.set_experiment(cfg.mlflow_experiment)
mlflow.tensorflow.autolog(log_models=False)  # Jak w starym kodzie - NIE disable=True

# ================================
# OPTYMALIZACJA 4: Szybkie kopiowanie danych lokalnie
# ================================
import shutil

def ensure_local_dataset(cfg):
    """Kopiuj dane z Google Drive do /content dla szybszego I/O"""
    src = cfg.primary_dataset_path if os.path.exists(cfg.primary_dataset_path) else cfg.fallback_dataset_path
    dst = "/content/dataset_local"

    if not os.path.exists(dst):
        print(" Kopiowanie danych do /content (szybsze I/O)...")
        start = time.time()
        shutil.copytree(src, dst)
        print(f" Skopiowano w {time.time()-start:.1f}s")
    else:
        print(" Dane już w /content")

    cfg.primary_dataset_path = dst
    cfg.fallback_dataset_path = dst

ensure_local_dataset(cfg)

# ================================
# OPTYMALIZACJA 5: Zoptymalizowany pipeline danych
# ================================
@tf.function
def normalize_image(image, label):
    """Normalizacja jako graf TF dla szybszości"""
    return tf.cast(image, tf.float32) / 255.0, label

@tf.function
def augment_image(image, label):
    """Augmentacja jako pojedynczy graf TF"""
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, 0.1)
    image = tf.image.random_contrast(image, 0.9, 1.1)
    # Rotacja jest kosztowna - użyj tylko jeśli naprawdę potrzebna
    return image, label

def create_datasets(cfg: Config, augmentation: bool = True):
    """Zoptymalizowany pipeline ładowania danych"""
    dataset_path = cfg.primary_dataset_path

    AUTOTUNE = tf.data.AUTOTUNE

    # OPTYMALIZACJA: Wczytaj wszystkie obrazy równolegle
    raw_train = tf.keras.utils.image_dataset_from_directory(
        os.path.join(dataset_path, "train"),
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",
        shuffle=True,
        seed=cfg.seed,
        interpolation='bilinear'  # Szybsza interpolacja
    )
    class_names = raw_train.class_names

    raw_val = tf.keras.utils.image_dataset_from_directory(
        os.path.join(dataset_path, "validation"),
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",
        shuffle=False,
        class_names=class_names,
        interpolation='bilinear'
    )

    raw_test = tf.keras.utils.image_dataset_from_directory(
        os.path.join(dataset_path, "test"),
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",
        shuffle=False,
        class_names=class_names,
        interpolation='bilinear'
    )

    # OPTYMALIZACJA 6: Efektywny pipeline z cache i prefetch
    def prepare_dataset(ds, training=False):
        # Normalizacja
        ds = ds.map(normalize_image, num_parallel_calls=AUTOTUNE)

        # Cache PRZED augmentacją (oszczędność czasu)
        if cfg.cache_data:
            ds = ds.cache()

        # Augmentacja tylko dla treningu
        if training and augmentation:
            ds = ds.map(augment_image, num_parallel_calls=AUTOTUNE)
            ds = ds.shuffle(1000, seed=cfg.seed, reshuffle_each_iteration=True)

        # Prefetch dla asynchronicznego ładowania
        ds = ds.prefetch(AUTOTUNE)

        return ds

    train_ds = prepare_dataset(raw_train, training=True)
    val_ds = prepare_dataset(raw_val, training=False)
    test_ds = prepare_dataset(raw_test, training=False)

    return train_ds, val_ds, test_ds, class_names

print(" Ładowanie datasetów...")
start = time.time()
train_ds, val_ds, test_ds, class_names = create_datasets(cfg, augmentation=True)
print(f" Datasety załadowane w {time.time()-start:.1f}s")

idx_to_class = {i: name for i, name in enumerate(class_names)}
n_classes = len(class_names)

# ================================
# OPTYMALIZACJA 7: Szybsze obliczanie wag klas
# ================================
print(" Obliczanie wag klas...")
start = time.time()

# Pobierz tylko 10 batchy dla wag (wystarczające dla estymacji)
y_samples = []
for i, (_, y) in enumerate(train_ds.take(10)):
    y_samples.append(y.numpy())
y_train = np.concatenate(y_samples).astype(int)

class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(y_train),
    y=y_train
)

class_weight_dict = {int(i): float(w) for i, w in enumerate(class_weights)}
print(f"✓ Wagi klas: {class_weight_dict} [{time.time()-start:.1f}s]")

# ================================
# OPTYMALIZACJA 8: Zoptymalizowany model
# ================================
def build_model_optuna(trial, img_size: Tuple[int, int], n_classes: int):
    """Zoptymalizowana architektura modelu"""
    n_conv_blocks = trial.suggest_int("n_conv_blocks", 2, 4)
    base_filters = trial.suggest_categorical("base_filters", [32, 64])
    kernel_size = trial.suggest_categorical("kernel_size", [3])  # 3x3 najszybsze
    dense_units = trial.suggest_categorical("dense_units", [128, 256])
    dropout_rate = trial.suggest_float("dropout_rate", 0.3, 0.5)
    lr = trial.suggest_float("lr", 1e-4, 1e-3, log=True)
    optimizer_name = trial.suggest_categorical("optimizer", ["adam"])  # Adam najszybszy

    model = Sequential(name="cnn_optuna")
    model.add(Input(shape=(*img_size, 3)))

    for i in range(n_conv_blocks):
        filters = base_filters * (2 ** i)
        model.add(Conv2D(filters, kernel_size, activation="relu", padding="same",
                        kernel_initializer='he_normal'))
        model.add(BatchNormalization())
        model.add(MaxPooling2D((2, 2)))

    model.add(Flatten())
    model.add(Dense(dense_units, activation="relu", kernel_initializer='he_normal'))
    model.add(Dropout(dropout_rate))
    model.add(Dense(n_classes, activation="softmax", dtype="float32"))

    return model, lr, optimizer_name

def compile_model(model, lr, optimizer_name):
    opt = Adam(learning_rate=lr)
    model.compile(
        optimizer=opt,
        loss="sparse_categorical_crossentropy",
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name="accuracy")],
        jit_compile=True
    )
    return model

# Callbacki i trenowanie
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
import time

# Callback ograniczający czas
class TimeLimitCallback(Callback):
    def __init__(self, max_seconds: int):
        super().__init__()
        self.max_seconds = max_seconds
        self.start_time = None

    def on_train_begin(self, logs=None):
        self.start_time = time.time()

    def on_epoch_end(self, epoch, logs=None):
        elapsed = time.time() - self.start_time
        if elapsed > self.max_seconds:
            self.model.stop_training = True
            print(f"\n[TimeLimit] Stop after {elapsed/3600:.2f} h "
                  f"(limit {self.max_seconds/3600:.2f} h).")

def fit_with_callbacks(model, train_ds, val_ds, epochs,
                       class_weight_dict, max_runtime_seconds):
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2,
                          min_lr=1e-6, verbose=1),
        TimeLimitCallback(max_seconds=max_runtime_seconds - 600),
    ]

    # Klasyczny log Keras – zawsze stabilny
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        class_weight=class_weight_dict,
        verbose=1,          # <- klasyczny log Keras
        callbacks=callbacks
    )
    return history

# Ewaluacja metryk dla tf.data.Dataset (etykiety int)
def plot_confusion_matrix(cm, class_labels, title="Confusion matrix", fname=None):
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(title)
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.tight_layout()
    if fname:
        plt.savefig(fname)
    plt.show(); plt.close()

def evaluate_dataset_metrics(model, dataset, name="val", class_names=None, log_to_mlflow=True, n_classes=None):
    y_true, y_pred_probs = [], []
    for x, y in dataset:
        y_true.append(y.numpy().astype(int))
        y_pred_probs.append(model.predict(x, verbose=0))
    y_true = np.concatenate(y_true).astype(int)
    y_pred_probs = np.vstack(y_pred_probs)
    y_pred = np.argmax(y_pred_probs, axis=1)

    acc = float((y_pred == y_true).mean())
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="macro", zero_division=0
    )
    # Wymuś pełny zakres etykiet w CM
    labels_full = list(range(n_classes if n_classes is not None else (len(class_names) if class_names else int(y_true.max())+1)))
    cm = confusion_matrix(y_true, y_pred, labels=labels_full)

    labels = class_names if class_names else labels_full
    print(f"[{name}] acc={acc:.4f} precision={precision:.4f} recall={recall:.4f} f1={f1:.4f}")
    cm_path = f"{name}_cm.png"
    plot_confusion_matrix(cm, class_labels=labels, title=f"Confusion matrix ({name})", fname=cm_path)

    if log_to_mlflow:
        mlflow.log_metric(f"{name}_acc", acc)
        mlflow.log_metric(f"{name}_precision", float(precision))
        mlflow.log_metric(f"{name}_recall", float(recall))
        mlflow.log_metric(f"{name}_f1", float(f1))
        if os.path.exists(cm_path):
            mlflow.log_artifact(cm_path, artifact_path="reports")

    return {"acc": acc, "precision": float(precision), "recall": float(recall), "f1": float(f1), "cm": cm}

import os, numpy as np, tensorflow as tf, matplotlib.pyplot as plt, cv2

# Optuna: objective
def objective(trial):
    # każdy trial logujemy jako run zagnieżdżony
    with mlflow.start_run(run_name=f"trial_{trial.number}", nested=True):
        model, lr, optimizer_name = build_model_optuna(trial, cfg.img_size, n_classes)
        model = compile_model(model, lr, optimizer_name)

        history = fit_with_callbacks(
            model,
            train_ds,
            val_ds,
            cfg.epochs,
            class_weight_dict,
            cfg.max_runtime_seconds
        )

        val_metrics = evaluate_dataset_metrics(
            model, val_ds, name="val", class_names=class_names,
            log_to_mlflow=False, n_classes=n_classes
        )
        score = val_metrics["f1"] if cfg.optimize_for == "f1" else val_metrics["acc"]

        # logowanie parametrów i metryk do MLflow
        mlflow.log_params(trial.params)
        for k, v in val_metrics.items():
            if k != "cm":
                mlflow.log_metric(f"val_{k}", v)

        return score


# Optuna tuning – uruchomienie całej optymalizacji
mlflow.end_run()  # upewnij się, że nie ma aktywnego runu
with mlflow.start_run(run_name="optuna_tuning"):
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=cfg.n_trials)
    mlflow.log_params(study.best_params)
    mlflow.log_metric("best_val_score", float(study.best_value))

print("Best trial:", study.best_params, "score:", study.best_value)


# Finalny model – trenowanie
best_trial = optuna.trial.FixedTrial(study.best_params)
final_model, lr, opt_name = build_model_optuna(best_trial, cfg.img_size, n_classes)
final_model = compile_model(final_model, lr, opt_name)
history = fit_with_callbacks(final_model, train_ds, val_ds,
                             cfg.epochs, class_weight_dict, cfg.max_runtime_seconds)


# Wykresy strat i accuracy
epochs_range = range(1, len(history.history['loss']) + 1)
plt.figure(figsize=(10,5))
plt.plot(epochs_range, history.history['loss'], label='Train loss')
plt.plot(epochs_range, history.history['val_loss'], label='Val loss')
plt.xlabel('Epoka'); plt.ylabel('Strata'); plt.title('Krzywe strat')
plt.xticks(list(epochs_range))
plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()

if 'accuracy' in history.history and 'val_accuracy' in history.history:
    epochs_range_acc = range(1, len(history.history['accuracy']) + 1)
    plt.figure(figsize=(10,5))
    plt.plot(epochs_range_acc, history.history['accuracy'], label='Train acc')
    plt.plot(epochs_range_acc, history.history['val_accuracy'], label='Val acc')
    plt.xlabel('Epoka'); plt.ylabel('Accuracy'); plt.title('Dokładność modelu')
    plt.xticks(list(epochs_range_acc))
    plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()





# =========================
# --- Test set recall and accuracy plots ---
# =========================
try:
    test_gen.reset()
    y_true = test_gen.classes
    y_pred_probs = final_model.predict(test_gen, verbose=0)
    y_pred = np.argmax(y_pred_probs, axis=1)

    classes = ['Glioma', 'Meningioma', 'Pituitary']
    report = classification_report(y_true, y_pred, target_names=classes, output_dict=True, zero_division=0)
    recalls = [report[c]['recall']*100 for c in classes]

    plt.style.use('seaborn-v0_8')
    fig, ax = plt.subplots(figsize=(8, 6))
    bars = ax.bar(classes, recalls, color=['#1f77b4','#ff7f0e','#2ca02c'])
    for bar, acc in zip(bars, recalls):
        ax.text(bar.get_x()+bar.get_width()/2, acc+1, f'{acc:.1f}%', ha='center', va='bottom', fontsize=12)
    ax.set_ylabel('Czułość (%)'); ax.set_title('Skuteczność wykrywania typów guzów mózgu'); ax.set_ylim(0, 105)
    plt.tight_layout(); plt.show()
except Exception as e:
    print(f"[PLOT] Skipping recall plot: {e}")

# Optional: accuracy plot
try:
    classes = ['Glioma', 'Meningioma', 'Pituitary']
    accuracies = [87.5, 92.3, 89.1]  # TODO: zamień na realne z test_gen jeśli chcesz
    plt.style.use('seaborn-v0_8')
    fig, ax = plt.subplots(figsize=(8, 6))
    bars = ax.bar(classes, accuracies, color=['#1f77b4','#ff7f0e','#2ca02c'])
    for bar, acc in zip(bars, accuracies):
        ax.text(bar.get_x()+bar.get_width()/2, acc+1, f'{acc:.1f}%', ha='center', va='bottom', fontsize=12)
    ax.set_ylabel('Dokładność (%)'); ax.set_title('Skuteczność klasyfikacji guzów mózgu'); ax.set_ylim(0, 105)
    plt.tight_layout(); plt.show()
except Exception as e:
    print(f"[PLOT2] Skipping accuracy plot: {e}")


# Heatmapa macierz pomyłek
def plot_confusion_matrix(cm, class_labels, title="Macierz pomyłek", fname=None):
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(title)
    plt.xlabel("Przewidziane")
    plt.ylabel("Rzeczywiste")
    plt.tight_layout()
    if fname:
        plt.savefig(fname)
    plt.show()
    plt.close()

# Zapis modelu
final_model.save(cfg.model_save_path)
if os.path.exists(cfg.model_save_path):
    mlflow.log_artifact(cfg.model_save_path)
print(f"✓ Model zapisany: {cfg.model_save_path}")


# Końcowa ewaluacja
print("\n" + "="*60)
print(" KOŃCOWA EWALUACJA")
print("="*60)

val_metrics = evaluate_dataset_metrics(
    final_model, val_ds, name="validation",
    class_names=class_names, n_classes=n_classes
)

test_metrics = evaluate_dataset_metrics(
    final_model, test_ds, name="test",
    class_names=class_names, n_classes=n_classes
)

print("\n✓ TRENING ZAKOŃCZONY SUKCESEM! ")

mlflow.end_run()
