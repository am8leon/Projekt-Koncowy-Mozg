!pip install -q "pydantic>=2.0" "pydantic-settings>=2.0" optuna mlflow tqdm seaborn scikit-learn nibabel pandas chardet tensorflow matplotlib opencv-python numpy
# Importy
import os, math, time, json, random
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cv2


import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
from tqdm.notebook import tqdm
from tqdm.keras import TqdmCallback

from sklearn.metrics import (
    precision_recall_fscore_support,
    confusion_matrix,
)
from sklearn.utils.class_weight import compute_class_weight

import mlflow
import mlflow.tensorflow
import optuna

from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Tuple, Optional, Dict

%matplotlib inline
plt.ion()

# Montowanie Dysku Google (Colab)
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Ustawienie katalogu MLflow na Dysku Google
mlflow.set_tracking_uri("file:/content/drive/MyDrive/mlruns")


# Konfiguracja projektu
class Config(BaseSettings):
    # Tryb działania
    mode: str = "optuna"                  # np. "optuna" albo "final"
    optimize_for: str = "f1"              # 'f1' lub 'accuracy'

    # Parametry uczenia
    n_trials: int = 2                  # liczba prób Optuny
    img_size: Tuple[int, int] = (96, 96)  # rozmiar wejściowych obrazów
    batch_size: int = 32
    epochs: int = 10
    seed: int = 42
    max_runtime_seconds: int = 12 * 60 * 60  # maksymalny czas treningu

    # Dane główne (foldery train/validation/test)
    primary_dataset_path: str = "/content/drive/MyDrive/dataset"
    fallback_dataset_path: str = "/content/drive/MyDrive/dataset_fallback"

    # Zewnętrzny zbiór (opcjonalny)
    external_dataset_dir: Optional[str] = None
    ext_max_samples: int = 1000

    # Walidacja BraTS – dwie lokalizacje (primary i fallback)
    brats_validation_paths_primary: Dict[str, str] = {
        "flair": "/content/drive/MyDrive/dataset/BraTS20_Validation_124_flair.nii",
        "t1":    "/content/drive/MyDrive/dataset/BraTS20_Validation_124_t1.nii",
        "t1ce":  "/content/drive/MyDrive/dataset/BraTS20_Validation_124_t1ce.nii",
        "t2":    "/content/drive/MyDrive/dataset/BraTS20_Validation_124_t2.nii",
    }
    brats_validation_paths_fallback: Dict[str, str] = {
        "flair": "/content/drive/MyDrive/brats/BraTS20_Validation_124_flair.nii",
        "t1":    "/content/drive/MyDrive/brats/BraTS20_Validation_124_t1.nii",
        "t1ce":  "/content/drive/MyDrive/brats/BraTS20_Validation_124_t1ce.nii",
        "t2":    "/content/drive/MyDrive/brats/BraTS20_Validation_124_t2.nii",
    }

    # Artefakty (gdzie zapisywać wyniki)
    model_save_path: str = "/content/drive/MyDrive/best_model_optuna.h5"
    best_params_path: str = "/content/drive/MyDrive/best_params_optuna.json"
    req_path: str = "/content/drive/MyDrive/dataset/requirements2.txt"

    # MLflow – eksperymenty i logi
    mlflow_experiment: str = "brain_mri"
    mlflow_dir: str = "/content/mlruns"

    # Grad-CAM
    gradcam_samples: int = 4

    # Pydantic
    model_config = SettingsConfigDict(env_file=".env", extra="ignore")

# Inicjalizacja
cfg = Config()


# Ustawienia globalne i reproducowalność
tf.keras.mixed_precision.set_global_policy('mixed_float16')

def set_seeds(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
set_seeds(cfg.seed)

# MLflow setup (lokalny backend plikowy)
os.makedirs(cfg.mlflow_dir, exist_ok=True)
mlflow.set_tracking_uri(f"file:{cfg.mlflow_dir}")
mlflow.set_experiment(cfg.mlflow_experiment)
mlflow.tensorflow.autolog(log_models=False)


# Dane: tf.data (spójność klas + etykiety int)
def create_datasets(cfg: Config, augmentation: bool = True):
    dataset_path = cfg.primary_dataset_path if os.path.exists(cfg.primary_dataset_path) else cfg.fallback_dataset_path

    # Train: najpierw, by ustalić class_names (kolejność)
    raw_train = tf.keras.utils.image_dataset_from_directory(
        os.path.join(dataset_path, "train"),
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",              # <- etykiety jako int
        shuffle=True,
        seed=cfg.seed,
    )
    class_names = raw_train.class_names

    # Val/Test: wymuszenie identycznego porządku klas
    raw_val = tf.keras.utils.image_dataset_from_directory(
        os.path.join(dataset_path, "validation"),
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",
        shuffle=False,
        class_names=class_names,
    )
    raw_test = tf.keras.utils.image_dataset_from_directory(
        os.path.join(dataset_path, "test"),
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",
        shuffle=False,
        class_names=class_names,
    )

    normalization = tf.keras.layers.Rescaling(1./255)
    aug = tf.keras.Sequential([
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.1),
        tf.keras.layers.RandomZoom(0.1),
        tf.keras.layers.RandomTranslation(0.1, 0.1),
    ]) if augmentation else None

    AUTOTUNE = tf.data.AUTOTUNE

    def prepare(ds, training=False):
        # cast do float32 w pipeline (AMP sobie poradzi niżej)
        ds = ds.map(lambda x, y: (tf.cast(x, tf.float32), y), num_parallel_calls=AUTOTUNE)
        ds = ds.map(lambda x, y: (normalization(x), y), num_parallel_calls=AUTOTUNE)
        if training and aug is not None:
            ds = ds.map(lambda x, y: (aug(x, training=True), y), num_parallel_calls=AUTOTUNE)
            ds = ds.shuffle(1000, seed=cfg.seed)
        ds = ds.cache().prefetch(AUTOTUNE)
        return ds

    train_ds = prepare(raw_train, training=True)
    val_ds   = prepare(raw_val, training=False)
    test_ds  = prepare(raw_test, training=False)
    return train_ds, val_ds, test_ds, class_names

train_ds, val_ds, test_ds, class_names = create_datasets(cfg, augmentation=True)
idx_to_class = {i: name for i, name in enumerate(class_names)}
n_classes = len(class_names)

# Wagi klas – prosto z etykiet int
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Wyciągnięcie wszystkich etykiet z datasetu
all_labels = [y.numpy() for _, y in train_ds.unbatch()]
y_train = np.array(all_labels)

# Obsługa obu trybów: int i one-hot
if y_train.ndim == 1:
    # label_mode="int" → etykiety są już liczbami całkowitymi
    y_train_classes = y_train.astype(int)
else:
    # label_mode="categorical" → one-hot → trzeba zrobić argmax
    y_train_classes = np.argmax(y_train, axis=1)

# Wyliczenie wag klas
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(y_train_classes),
    y=y_train_classes
)

# Zamiana na słownik {klasa: waga}
class_weight_dict = {int(i): float(w) for i, w in enumerate(class_weights)}
print("class_weight:", class_weight_dict)


# External (opcjonalnie, bez wymuszania class_names)
def try_create_external_dataset(cfg: Config):
    if not cfg.external_dataset_dir or not os.path.exists(cfg.external_dataset_dir):
        return None
    ext_raw = tf.keras.utils.image_dataset_from_directory(
        cfg.external_dataset_dir,
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",
        shuffle=False
    )
    AUTOTUNE = tf.data.AUTOTUNE
    ext_ds = ext_raw.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y), num_parallel_calls=AUTOTUNE).cache().prefetch(AUTOTUNE)
    return ext_ds

ext_ds = try_create_external_dataset(cfg)

# Podgląd kilku obrazów
images, labels = next(iter(train_ds))  # pobierz jeden batch
images, labels = images.numpy(), labels.numpy()

fig, axes = plt.subplots(1, min(5, images.shape[0]), figsize=(15, 5))
for i in range(min(5, images.shape[0])):
    axes[i].imshow(images[i].astype(np.float32))
    axes[i].axis('off')
    axes[i].set_title(f'Label: {class_names[int(labels[i])]}')
plt.tight_layout()
plt.show()


# Model (Optuna)
def build_model_optuna(trial, img_size: Tuple[int, int], n_classes: int):
    n_conv_blocks = trial.suggest_int("n_conv_blocks", 2, 3)
    base_filters = trial.suggest_categorical("base_filters", [16, 32, 64])
    kernel_size = trial.suggest_categorical("kernel_size", [3, 5])
    dense_units = trial.suggest_categorical("dense_units", [64, 128, 256])
    dropout_rate = trial.suggest_float("dropout_rate", 0.2, 0.6)
    lr = trial.suggest_float("lr", 1e-5, 1e-3, log=True)
    optimizer_name = trial.suggest_categorical("optimizer", ["adam", "rmsprop"])
    use_batchnorm = trial.suggest_categorical("batchnorm", [True, False])

    model = Sequential(name="cnn_optuna")
    model.add(Input(shape=(*img_size, 3)))
    for i in range(n_conv_blocks):
        filters = base_filters * (2 ** i)
        model.add(Conv2D(filters, (kernel_size, kernel_size), activation="relu", padding="same"))
        if use_batchnorm:
            model.add(BatchNormalization())
        model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(dense_units, activation="relu"))
    model.add(Dropout(dropout_rate))
    # Zostawiamy float32 na wyjściu (stabilne metryki przy AMP)
    model.add(Dense(n_classes, activation="softmax", dtype="float32"))
    return model, lr, optimizer_name

def compile_model(model, lr, optimizer_name):
    opt = Adam(learning_rate=lr) if optimizer_name == "adam" else RMSprop(learning_rate=lr)
    # SPARSE loss + SparseCategoricalAccuracy (bo etykiety są int)
    model.compile(
        optimizer=opt,
        loss="sparse_categorical_crossentropy",
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name="accuracy")]
    )
    return model
# Callbacki i trenowanie
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
import time

# Callback ograniczający czas
class TimeLimitCallback(Callback):
    def __init__(self, max_seconds: int):
        super().__init__()
        self.max_seconds = max_seconds
        self.start_time = None

    def on_train_begin(self, logs=None):
        self.start_time = time.time()

    def on_epoch_end(self, epoch, logs=None):
        elapsed = time.time() - self.start_time
        if elapsed > self.max_seconds:
            self.model.stop_training = True
            print(f"\n[TimeLimit] Stop after {elapsed/3600:.2f} h "
                  f"(limit {self.max_seconds/3600:.2f} h).")

def fit_with_callbacks(model, train_ds, val_ds, epochs,
                       class_weight_dict, max_runtime_seconds):
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2,
                          min_lr=1e-6, verbose=1),
        TimeLimitCallback(max_seconds=max_runtime_seconds - 600),
    ]

    # Klasyczny log Keras – zawsze stabilny
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        class_weight=class_weight_dict,
        verbose=1,          # <- klasyczny log Keras
        callbacks=callbacks
    )
    return history

# Ewaluacja metryk dla tf.data.Dataset (etykiety int)
def plot_confusion_matrix(cm, class_labels, title="Confusion matrix", fname=None):
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(title)
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.tight_layout()
    if fname:
        plt.savefig(fname)
    plt.show(); plt.close()

def evaluate_dataset_metrics(model, dataset, name="val", class_names=None, log_to_mlflow=True, n_classes=None):
    y_true, y_pred_probs = [], []
    for x, y in dataset:
        y_true.append(y.numpy().astype(int))
        y_pred_probs.append(model.predict(x, verbose=0))
    y_true = np.concatenate(y_true).astype(int)
    y_pred_probs = np.vstack(y_pred_probs)
    y_pred = np.argmax(y_pred_probs, axis=1)

    acc = float((y_pred == y_true).mean())
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="macro", zero_division=0
    )
    # Wymuś pełny zakres etykiet w CM
    labels_full = list(range(n_classes if n_classes is not None else (len(class_names) if class_names else int(y_true.max())+1)))
    cm = confusion_matrix(y_true, y_pred, labels=labels_full)

    labels = class_names if class_names else labels_full
    print(f"[{name}] acc={acc:.4f} precision={precision:.4f} recall={recall:.4f} f1={f1:.4f}")
    cm_path = f"{name}_cm.png"
    plot_confusion_matrix(cm, class_labels=labels, title=f"Confusion matrix ({name})", fname=cm_path)

    if log_to_mlflow:
        mlflow.log_metric(f"{name}_acc", acc)
        mlflow.log_metric(f"{name}_precision", float(precision))
        mlflow.log_metric(f"{name}_recall", float(recall))
        mlflow.log_metric(f"{name}_f1", float(f1))
        if os.path.exists(cm_path):
            mlflow.log_artifact(cm_path, artifact_path="reports")

    return {"acc": acc, "precision": float(precision), "recall": float(recall), "f1": float(f1), "cm": cm}

import os, numpy as np, tensorflow as tf, matplotlib.pyplot as plt, cv2

# Optuna: objective
def objective(trial):
    # każdy trial logujemy jako run zagnieżdżony
    with mlflow.start_run(run_name=f"trial_{trial.number}", nested=True):
        model, lr, optimizer_name = build_model_optuna(trial, cfg.img_size, n_classes)
        model = compile_model(model, lr, optimizer_name)

        history = fit_with_callbacks(
            model,
            train_ds,
            val_ds,
            cfg.epochs,
            class_weight_dict,
            cfg.max_runtime_seconds
        )

        val_metrics = evaluate_dataset_metrics(
            model, val_ds, name="val", class_names=class_names,
            log_to_mlflow=False, n_classes=n_classes
        )
        score = val_metrics["f1"] if cfg.optimize_for == "f1" else val_metrics["acc"]

        # logowanie parametrów i metryk do MLflow
        mlflow.log_params(trial.params)
        for k, v in val_metrics.items():
            if k != "cm":
                mlflow.log_metric(f"val_{k}", v)

        return score


# Optuna tuning – uruchomienie całej optymalizacji
mlflow.end_run()  # upewnij się, że nie ma aktywnego runu
with mlflow.start_run(run_name="optuna_tuning"):
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=cfg.n_trials)
    mlflow.log_params(study.best_params)
    mlflow.log_metric("best_val_score", float(study.best_value))

print("Best trial:", study.best_params, "score:", study.best_value)


# Finalny model – trenowanie
best_trial = optuna.trial.FixedTrial(study.best_params)
final_model, lr, opt_name = build_model_optuna(best_trial, cfg.img_size, n_classes)
final_model = compile_model(final_model, lr, opt_name)
history = fit_with_callbacks(final_model, train_ds, val_ds,
                             cfg.epochs, class_weight_dict, cfg.max_runtime_seconds)

# Grad-CAM (dopiero po trenowaniu)
last_conv_layer_name = None
for layer in reversed(final_model.layers):
    if isinstance(layer, tf.keras.layers.Conv2D):
        last_conv_layer_name = layer.name
        break

if last_conv_layer_name is None:
    print(" Brak warstwy Conv2D w modelu – Grad-CAM nie zadziała")
else:
    # Warm-up: przepuść batch, żeby model miał wyjścia
    _ = final_model.predict(val_ds.take(1), verbose=0)

    # weź batch z walidacji
    images, labels = next(iter(val_ds))
    img_array = np.expand_dims(images[0], axis=0)

    # wygeneruj heatmapę
    heatmap = make_gradcam_heatmap(img_array, final_model, last_conv_layer_name)

    # przygotuj obraz wejściowy (uint8)
    orig_img = np.uint8(images[0].numpy() * 255)

    # nałóż heatmapę
    overlay = overlay_heatmap(orig_img, heatmap)

    plt.imshow(overlay)
    plt.axis("off")
    plt.title("Grad-CAM (Optuna model)")
    plt.show()

# Wykresy strat i accuracy
epochs_range = range(1, len(history.history['loss']) + 1)
plt.figure(figsize=(10,5))
plt.plot(epochs_range, history.history['loss'], label='Train loss')
plt.plot(epochs_range, history.history['val_loss'], label='Val loss')
plt.xlabel('Epoka'); plt.ylabel('Strata'); plt.title('Krzywe strat')
plt.xticks(list(epochs_range))
plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()

if 'accuracy' in history.history and 'val_accuracy' in history.history:
    epochs_range_acc = range(1, len(history.history['accuracy']) + 1)
    plt.figure(figsize=(10,5))
    plt.plot(epochs_range_acc, history.history['accuracy'], label='Train acc')
    plt.plot(epochs_range_acc, history.history['val_accuracy'], label='Val acc')
    plt.xlabel('Epoka'); plt.ylabel('Accuracy'); plt.title('Dokładność modelu')
    plt.xticks(list(epochs_range_acc))
    plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()

# Zapis i logowanie modelu
final_model.save(cfg.model_save_path)
if os.path.exists(cfg.model_save_path):
    mlflow.log_artifact(cfg.model_save_path)


# Walidacja i test – metryki + macierz pomyłek
val_metrics = evaluate_dataset_metrics(final_model, val_ds, name="validation", class_names=class_names, n_classes=n_classes)
test_metrics = evaluate_dataset_metrics(final_model, test_ds, name="test", class_names=class_names, n_classes=n_classes)

# Predykcje na teście do statystyk
test_probs = final_model.predict(test_ds, verbose=0)
test_pred = np.argmax(test_probs, axis=1)

y_true_batches = []
for _, y in test_ds:
    y_true_batches.append(y.numpy().astype(int))
test_true = np.concatenate(y_true_batches).astype(int)

# Histogram liczby próbek per przewidziana klasa
plt.figure(figsize=(8,4))
counts = np.bincount(test_pred, minlength=n_classes)
sns.barplot(x=class_names, y=counts)
plt.title("Rozkład przewidzianych klas (test)")
plt.ylabel("Liczba próbek"); plt.xlabel("Klasa")
plt.tight_layout(); plt.show()

# Bezpieczne domknięcie runu MLflow (jeśli otwarty)
try:
    mlflow.end_run()
except Exception:
    pass
# Heatmapa macierz pomyłek
def plot_confusion_matrix(cm, class_labels, title="Macierz pomyłek", fname=None):
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(title)
    plt.xlabel("Przewidziane")
    plt.ylabel("Rzeczywiste")
    plt.tight_layout()
    if fname:
        plt.savefig(fname)
    plt.show()
    plt.close()

# Histogram pewności poprawnych vs błędnych predykcji
max_conf = test_probs.max(axis=1)
correct_mask = (test_pred == test_true)

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
sns.histplot(max_conf[correct_mask], bins=20, color="green", kde=False)
plt.title("Pewność (poprawne)"); plt.xlabel("Prawdopodobieństwo"); plt.ylabel("Liczba")

plt.subplot(1,2,2)
sns.histplot(max_conf[~correct_mask], bins=20, color="red", kde=False)
plt.title("Pewność (błędne)"); plt.xlabel("Prawdopodobieństwo"); plt.ylabel("Liczba")

plt.tight_layout(); plt.show()

# Wykres F1 na walidacji (finalny trening)
plt.figure(figsize=(8,4))
plt.plot(range(1, len(final_f1_cb.f1_history)+1), final_f1_cb.f1_history, marker='o')
plt.xlabel('Epoka')
plt.ylabel('Makro F1 (val)')
plt.title('F1 na walidacji podczas treningu')
plt.grid(True, alpha=0.3)
plt.show()



