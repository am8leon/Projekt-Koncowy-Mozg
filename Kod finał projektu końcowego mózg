!pip install -q "pydantic>=2.0" "pydantic-settings>=2.0" optuna mlflow tqdm seaborn scikit-learn nibabel pandas chardet tensorflow matplotlib opencv-python numpy
# Importy
import os, math, time, json, random
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cv2


import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
from tqdm.notebook import tqdm
from tqdm.keras import TqdmCallback

from sklearn.metrics import (
    precision_recall_fscore_support,
    confusion_matrix,
)
from sklearn.utils.class_weight import compute_class_weight

import mlflow
import mlflow.tensorflow
import optuna

from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Tuple, Optional, Dict

%matplotlib inline
plt.ion()

# Mounting Google Drive (Colab)
from google.colab import drive
drive.mount('/content/drive', force_remount=True)


# Setting up the MLflow directory in Google Drive
mlflow.set_tracking_uri("file:/content/drive/MyDrive/mlruns")


# Project Configuration
class Config(BaseSettings):
    # Operation mode
    mode: str = "optuna"                  # np. "optuna" 
    optimize_for: str = "f1"              # 'f1' lub 'accuracy'

    # Learning parameters
    n_trials: int = 2                 # # number of Optuna attempts
    img_size: Tuple[int, int] = (64, 64)  # size of input images
    batch_size: int = 32
    epochs: int = 5
    seed: int = 42
    max_runtime_seconds: int = 12 * 60 * 60  # # maximum training time

    
    # Master data (train/validation/test folders)
    primary_dataset_path: str = "/content/drive/MyDrive/dataset"
    fallback_dataset_path: str = "/content/drive/MyDrive/dataset_fallback"

    # External set (optional)
    external_dataset_dir: Optional[str] = None
    ext_max_samples: int = 300

    # BraTS Validation – Two Locations (Primary and Fallback)
    brats_validation_paths_primary: Dict[str, str] = {
        "flair": "/content/drive/MyDrive/dataset/BraTS20_Validation_124_flair.nii",
        "t1":    "/content/drive/MyDrive/dataset/BraTS20_Validation_124_t1.nii",
        "t1ce":  "/content/drive/MyDrive/dataset/BraTS20_Validation_124_t1ce.nii",
        "t2":    "/content/drive/MyDrive/dataset/BraTS20_Validation_124_t2.nii",
    }
    brats_validation_paths_fallback: Dict[str, str] = {
        "flair": "/content/drive/MyDrive/brats/BraTS20_Validation_124_flair.nii",
        "t1":    "/content/drive/MyDrive/brats/BraTS20_Validation_124_t1.nii",
        "t1ce":  "/content/drive/MyDrive/brats/BraTS20_Validation_124_t1ce.nii",
        "t2":    "/content/drive/MyDrive/brats/BraTS20_Validation_124_t2.nii",
    }

    # Artifacts (where to save results)
    model_save_path: str = "/content/drive/MyDrive/best_model_optuna.h5"
    best_params_path: str = "/content/drive/MyDrive/best_params_optuna.json"
    req_path: str = "/content/drive/MyDrive/dataset/requirements2.txt"

    # MLflow – experiments and logs
    mlflow_experiment: str = "brain_mri"
    mlflow_dir: str = "/content/mlruns"

  
    # Pydantic
    model_config = SettingsConfigDict(env_file=".env", extra="ignore")

# Initialization
cfg = Config()


# Global Settings
tf.keras.mixed_precision.set_global_policy('mixed_float16')

def set_seeds(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
set_seeds(cfg.seed)

# MLflow setup (local file backend)
os.makedirs(cfg.mlflow_dir, exist_ok=True)
mlflow.set_tracking_uri(f"file:{cfg.mlflow_dir}")
mlflow.set_experiment(cfg.mlflow_experiment)
mlflow.tensorflow.autolog(log_models=False)


# Data: tf.data (class consistency + int labels)
def create_datasets(cfg: Config, augmentation: bool = True):
    dataset_path = cfg.primary_dataset_path if os.path.exists(cfg.primary_dataset_path) else cfg.fallback_dataset_path

    # Train: first to establish class_names (order)
    raw_train = tf.keras.utils.image_dataset_from_directory(
        os.path.join(dataset_path, "train"),
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",              
        shuffle=True,
        seed=cfg.seed,
    )
    class_names = raw_train.class_names

    # Val/Test: force identical class order
    raw_val = tf.keras.utils.image_dataset_from_directory(
        os.path.join(dataset_path, "validation"),
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",
        shuffle=False,
        class_names=class_names,
    )
    raw_test = tf.keras.utils.image_dataset_from_directory(
        os.path.join(dataset_path, "test"),
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",
        shuffle=False,
        class_names=class_names,
    )

    normalization = tf.keras.layers.Rescaling(1./255)
    aug = tf.keras.Sequential([
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.1),
        tf.keras.layers.RandomZoom(0.1),
        tf.keras.layers.RandomTranslation(0.1, 0.1),
    ]) if augmentation else None

    AUTOTUNE = tf.data.AUTOTUNE
    

    def prepare(ds, training=False):
        # cast do float32 w pipeline (AMP sobie poradzi niżej)
        ds = ds.map(lambda x, y: (tf.cast(x, tf.float32), y), num_parallel_calls=AUTOTUNE)
        ds = ds.map(lambda x, y: (normalization(x), y), num_parallel_calls=AUTOTUNE)
        if training and aug is not None:
            ds = ds.map(lambda x, y: (aug(x, training=True), y), num_parallel_calls=AUTOTUNE)
            ds = ds.shuffle(1000, seed=cfg.seed)
        ds = ds.cache().prefetch(AUTOTUNE)
        return ds

    train_ds = prepare(raw_train, training=True)
    val_ds   = prepare(raw_val, training=False)
    test_ds  = prepare(raw_test, training=False)
    return train_ds, val_ds, test_ds, class_names

train_ds, val_ds, test_ds, class_names = create_datasets(cfg, augmentation=True)
idx_to_class = {i: name for i, name in enumerate(class_names)}
n_classes = len(class_names)


# Class weights – straight from int labels
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Extract all labels from the dataset
all_labels = [y.numpy() for _, y in train_ds.unbatch()]
y_train = np.array(all_labels)

# Supports both int and one-hot modes
if y_train.ndim == 1:
    # label_mode="int" → etykiety są już liczbami całkowitymi
    y_train_classes = y_train.astype(int)
else:
    # label_mode="categorical" → one-hot → trzeba zrobić argmax
    y_train_classes = np.argmax(y_train, axis=1)

# Calculation of class weights
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(y_train_classes),
    y=y_train_classes
)

# Convert to dictionary {class: weight}
class_weight_dict = {int(i): float(w) for i, w in enumerate(class_weights)}
print("class_weight:", class_weight_dict)

# External (optional, no class_names forced)
def try_create_external_dataset(cfg: Config):
    if not cfg.external_dataset_dir or not os.path.exists(cfg.external_dataset_dir):
        return None
    ext_raw = tf.keras.utils.image_dataset_from_directory(
        cfg.external_dataset_dir,
        image_size=cfg.img_size,
        batch_size=cfg.batch_size,
        label_mode="int",
        shuffle=False
    )
    AUTOTUNE = tf.data.AUTOTUNE
    ext_ds = ext_raw.map(lambda x, y: (tf.cast(x, tf.float32)/255.0, y), num_parallel_calls=AUTOTUNE).cache().prefetch(AUTOTUNE)
    return ext_ds

ext_ds = try_create_external_dataset(cfg)

# Preview multiple images
images, labels = next(iter(train_ds))  # pobierz jeden batch
images, labels = images.numpy(), labels.numpy()

fig, axes = plt.subplots(1, min(5, images.shape[0]), figsize=(15, 5))
for i in range(min(5, images.shape[0])):
    axes[i].imshow(images[i].astype(np.float32))
    axes[i].axis('off')
    axes[i].set_title(f'Label: {class_names[int(labels[i])]}')
plt.tight_layout()
plt.show()


# Model (Optuna)
def build_model_optuna(trial, img_size: Tuple[int, int], n_classes: int):
    n_conv_blocks = trial.suggest_int("n_conv_blocks", 2, 3)
    base_filters = trial.suggest_categorical("base_filters", [16, 32, 64])
    kernel_size = trial.suggest_categorical("kernel_size", [3, 5])
    dense_units = trial.suggest_categorical("dense_units", [64, 128, 256])
    dropout_rate = trial.suggest_float("dropout_rate", 0.2, 0.6)
    lr = trial.suggest_float("lr", 1e-5, 1e-3, log=True)
    optimizer_name = trial.suggest_categorical("optimizer", ["adam", "rmsprop"])
    use_batchnorm = trial.suggest_categorical("batchnorm", [True, False])

    model = Sequential(name="cnn_optuna")
    model.add(Input(shape=(*img_size, 3)))
    for i in range(n_conv_blocks):
        filters = base_filters * (2 ** i)
        model.add(Conv2D(filters, (kernel_size, kernel_size), activation="relu", padding="same"))
        if use_batchnorm:
            model.add(BatchNormalization())
        model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(dense_units, activation="relu"))
    model.add(Dropout(dropout_rate))
    # Zostawiamy float32 na wyjściu (stabilne metryki przy AMP)
    model.add(Dense(n_classes, activation="softmax", dtype="float32"))
    return model, lr, optimizer_name

def compile_model(model, lr, optimizer_name):
    opt = Adam(learning_rate=lr) if optimizer_name == "adam" else RMSprop(learning_rate=lr)
    # SPARSE loss + SparseCategoricalAccuracy (bo etykiety są int)
    model.compile(
        optimizer=opt,
        loss="sparse_categorical_crossentropy",
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name="accuracy")]
    )
    return model

# Callbacks and coaching
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
import time

# Time limit callback
class TimeLimitCallback(Callback):
    def __init__(self, max_seconds: int):
        super().__init__()
        self.max_seconds = max_seconds
        self.start_time = None

    def on_train_begin(self, logs=None):
        self.start_time = time.time()

    def on_epoch_end(self, epoch, logs=None):
        elapsed = time.time() - self.start_time
        if elapsed > self.max_seconds:
            self.model.stop_training = True
            print(f"\n[TimeLimit] Stop after {elapsed/3600:.2f} h "
                  f"(limit {self.max_seconds/3600:.2f} h).")

def fit_with_callbacks(model, train_ds, val_ds, epochs,
                       class_weight_dict, max_runtime_seconds):
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2,
                          min_lr=1e-6, verbose=1),
        TimeLimitCallback(max_seconds=max_runtime_seconds - 600),
    ]

    # Classic Keras log – always stable
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=epochs,
        class_weight=class_weight_dict,
        verbose=1,          # <- klasyczny log Keras
        callbacks=callbacks
    )
    return history

# Evaluate metrics for tf.data.Dataset (int labels)
def plot_confusion_matrix(cm, class_labels, title="Confusion matrix", fname=None):
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(title)
    plt.xlabel("Predicted"); plt.ylabel("True")
    plt.tight_layout()
    if fname:
        plt.savefig(fname)
    plt.show(); plt.close()

def evaluate_dataset_metrics(model, dataset, name="val", class_names=None, log_to_mlflow=True, n_classes=None):
    y_true, y_pred_probs = [], []
    for x, y in dataset:
        y_true.append(y.numpy().astype(int))
        y_pred_probs.append(model.predict(x, verbose=0))
    y_true = np.concatenate(y_true).astype(int)
    y_pred_probs = np.vstack(y_pred_probs)
    y_pred = np.argmax(y_pred_probs, axis=1)

    acc = float((y_pred == y_true).mean())
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="macro", zero_division=0
    )
   
    # Force full label range in CM
    labels_full = list(range(n_classes if n_classes is not None else (len(class_names) if class_names else int(y_true.max())+1)))
    cm = confusion_matrix(y_true, y_pred, labels=labels_full)

    labels = class_names if class_names else labels_full
    print(f"[{name}] acc={acc:.4f} precision={precision:.4f} recall={recall:.4f} f1={f1:.4f}")
    cm_path = f"{name}_cm.png"
    plot_confusion_matrix(cm, class_labels=labels, title=f"Confusion matrix ({name})", fname=cm_path)

    if log_to_mlflow:
        mlflow.log_metric(f"{name}_acc", acc)
        mlflow.log_metric(f"{name}_precision", float(precision))
        mlflow.log_metric(f"{name}_recall", float(recall))
        mlflow.log_metric(f"{name}_f1", float(f1))
        if os.path.exists(cm_path):
            mlflow.log_artifact(cm_path, artifact_path="reports")

    return {"acc": acc, "precision": float(precision), "recall": float(recall), "f1": float(f1), "cm": cm}

import os, numpy as np, tensorflow as tf, matplotlib.pyplot as plt, cv2

# Optuna: objective
def objective(trial):
    # każdy trial logujemy jako run zagnieżdżony
    with mlflow.start_run(run_name=f"trial_{trial.number}", nested=True):
        model, lr, optimizer_name = build_model_optuna(trial, cfg.img_size, n_classes)
        model = compile_model(model, lr, optimizer_name)

        history = fit_with_callbacks(
            model,
            train_ds,
            val_ds,
            cfg.epochs,
            class_weight_dict,
            cfg.max_runtime_seconds
        )

        val_metrics = evaluate_dataset_metrics(
            model, val_ds, name="val", class_names=class_names,
            log_to_mlflow=False, n_classes=n_classes
        )
        score = val_metrics["f1"] if cfg.optimize_for == "f1" else val_metrics["acc"]

        # logging parameters and metrics to MLflow
        mlflow.log_params(trial.params)
        for k, v in val_metrics.items():
            if k != "cm":
                mlflow.log_metric(f"val_{k}", v)

        return score

# Optuna tuning – running the entire optimization
mlflow.end_run()  
with mlflow.start_run(run_name="optuna_tuning"):
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=cfg.n_trials)
    mlflow.log_params(study.best_params)
    mlflow.log_metric("best_val_score", float(study.best_value))

print("Best trial:", study.best_params, "score:", study.best_value)

# Final model – training
best_trial = optuna.trial.FixedTrial(study.best_params)
final_model, lr, opt_name = build_model_optuna(best_trial, cfg.img_size, n_classes)
final_model = compile_model(final_model, lr, opt_name)
history = fit_with_callbacks(final_model, train_ds, val_ds,
                             cfg.epochs, class_weight_dict, cfg.max_runtime_seconds)


# Loss and accuracy charts
epochs_range = range(1, len(history.history['loss']) + 1)
plt.figure(figsize=(10,5))
plt.plot(epochs_range, history.history['loss'], label='Train loss')
plt.plot(epochs_range, history.history['val_loss'], label='Val loss')
plt.xlabel('Epoka'); plt.ylabel('Strata'); plt.title('Krzywe strat')
plt.xticks(list(epochs_range))
plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()

if 'accuracy' in history.history and 'val_accuracy' in history.history:
    epochs_range_acc = range(1, len(history.history['accuracy']) + 1)
    plt.figure(figsize=(10,5))
    plt.plot(epochs_range_acc, history.history['accuracy'], label='Train acc')
    plt.plot(epochs_range_acc, history.history['val_accuracy'], label='Val acc')
    plt.xlabel('Epoka'); plt.ylabel('Accuracy'); plt.title('Dokładność modelu')
    plt.xticks(list(epochs_range_acc))
    plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()

# Saving and logging the model
final_model.save(cfg.model_save_path)
if os.path.exists(cfg.model_save_path):
    mlflow.log_artifact(cfg.model_save_path)

# Validation and test – metrics + error matrix
val_metrics = evaluate_dataset_metrics(final_model, val_ds, name="validation", class_names=class_names, n_classes=n_classes)
test_metrics = evaluate_dataset_metrics(final_model, test_ds, name="test", class_names=class_names, n_classes=n_classes)

# Predictions on the statistics test
test_probs = final_model.predict(test_ds, verbose=0)
test_pred = np.argmax(test_probs, axis=1)

y_true_batches = []
for _, y in test_ds:
    y_true_batches.append(y.numpy().astype(int))
test_true = np.concatenate(y_true_batches).astype(int)


# Histogram of the number of samples per predicted class
plt.figure(figsize=(8,4))
counts = np.bincount(test_pred, minlength=n_classes)
sns.barplot(x=class_names, y=counts)
plt.title("Rozkład przewidzianych klas (test)")
plt.ylabel("Liczba próbek"); plt.xlabel("Klasa")
plt.tight_layout(); plt.show()


# Heatmap confusion matrix
def plot_confusion_matrix(cm, class_labels, title="Macierz pomyłek", fname=None):
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
    plt.title(title)
    plt.xlabel("Przewidziane")
    plt.ylabel("Rzeczywiste")
    plt.tight_layout()
    if fname:
        plt.savefig(fname)
    plt.show()
    plt.close()

# Safely close MLflow run (if open)
try:
    mlflow.end_run()
except Exception:
    pass

