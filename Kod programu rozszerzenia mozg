!pip install mlflow==2.12.1 azureml-mlflow==1.55.0 pandas==2.2.2 chardet==5.2.0 uvicorn==0.29.0 fastapi==0.110.0 seaborn==0.13.2
import os
import io
import cv2
import json
import base64
import chardet
import pandas as pd
import mlflow
import nibabel as nib
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
from fastapi import FastAPI, UploadFile, File, HTTPException
from starlette.responses import JSONResponse
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score



drive.mount('/content/drive')

IMG_SIZE = (128, 128)
BATCH_SIZE = 32
EPOCHS = 10

PRIMARY_DATASET_PATH = "/content/drive/MyDrive/dataset"
FALLBACK_DATASET_PATH = "/content/drive/MyDrive/dataset_fallback"

DATASET_PATH = PRIMARY_DATASET_PATH if os.path.exists(os.path.join(PRIMARY_DATASET_PATH, "train")) else FALLBACK_DATASET_PATH
if not os.path.exists(DATASET_PATH):
    raise FileNotFoundError(f"Nie znaleziono zbioru danych ani w {PRIMARY_DATASET_PATH} ani w {FALLBACK_DATASET_PATH}")

# Narzędzia
def read_csv_with_fallback(filepath, encodings=["utf-8", "utf-8-sig", "latin1", "cp1250", "iso-8859-2"]):
    with open(filepath, 'rb') as f:
        result = chardet.detect(f.read(10000))
    for enc in encodings:
        try:
            with open(filepath, 'r', encoding=enc) as f:
                sample = f.read(1024)
                delimiter = ';' if ';' in sample else ','
            df = pd.read_csv(filepath, encoding=enc, sep=delimiter)
            return df
        except Exception:
            continue
    raise ValueError(f"Nie udało się wczytać pliku {filepath}")

def load_nii_volume(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Plik nie istnieje: {path}")
    return nib.load(path).get_fdata()

def preprocess_brats_volume(vol_dict, modalities):
    arrays = [load_nii_volume(vol_dict[m]) for m in modalities]
    slices = [arr[:, :, arr.shape[2] // 2] for arr in arrays]
    img_stack = np.stack(slices, axis=-1)
    img_resized = cv2.resize(img_stack, IMG_SIZE)
    img_norm = (img_resized - np.min(img_resized)) / (np.ptp(img_resized) + 1e-8)
    return img_norm.astype(np.float32)

def safe_flow(datagen, path, name):
    if not os.path.exists(path) or len(os.listdir(path)) == 0:
        raise FileNotFoundError(f"Brak danych w katalogu {path} dla {name}")
    return datagen.flow_from_directory(
        path,
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode="categorical"
    )

def create_generators(datagen):
    return (
        safe_flow(datagen, os.path.join(DATASET_PATH, "train"), "train"),
        safe_flow(ImageDataGenerator(rescale=1./255), os.path.join(DATASET_PATH, "validation"), "validation"),
        safe_flow(ImageDataGenerator(rescale=1./255), os.path.join(DATASET_PATH, "test"), "test")
    )

def build_model(variant="baseline", lr=1e-3):
    layers = []
    for filters in [32, 64, 128]:
        layers.append(Conv2D(filters, (3, 3), activation="relu", padding="same"))
        if "batchnorm" in variant:
            layers.append(BatchNormalization())
        layers.append(MaxPooling2D((2, 2)))
    layers.append(Flatten())
    layers.append(Dense(128, activation="relu"))
    if "dropout" in variant:
        layers.append(Dropout(0.5))
    layers.append(Dense(3, activation="softmax"))

    model = Sequential(layers)
    model.compile(optimizer=Adam(learning_rate=lr), loss="categorical_crossentropy", metrics=["accuracy"])
    return model
# Grad-CAM
def make_gradcam_heatmap(img_array, model, last_conv_layer_name):
    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        class_idx = tf.argmax(predictions[0])
        loss = predictions[:, class_idx]
    grads = tape.gradient(loss, conv_outputs)[0]
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))
    conv_outputs = conv_outputs[0]
    for i in range(pooled_grads.shape[-1]):
        conv_outputs[:, :, i] *= pooled_grads[i]
    heatmap = tf.reduce_mean(conv_outputs, axis=-1).numpy()
    return cv2.resize(np.maximum(heatmap, 0) / (np.max(heatmap) + 1e-8), IMG_SIZE)

def overlay_heatmap(orig_img, heatmap, alpha=0.4):
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    return cv2.addWeighted(orig_img, 1 - alpha, heatmap, alpha, 0)

# Pobieranie ścieżki do katalogu z plikami
current_dir = os.getcwd()

def get_filepath(filename):
    """Funkcja zwraca pełną ścieżkę do pliku"""
    return os.path.join(current_dir, 'dane', filename)

# Wczytanie plików CSV
csv_files = ["name_mapping_validation_data.csv", "name_mapping.csv", "survival_evaluation.csv", "survival_info.csv"]
for csv in csv_files:
    full_path = os.path.join(DATASET_PATH, csv)
    if os.path.exists(full_path):
        df = read_csv_with_fallback(full_path)
    else:
        print(f"Brak pliku {csv}")


#  MLflow + Pętla Treningowa
mlflow.set_experiment("brain_mri_classification")
results = {}
best_val_acc = 0
best_run_id = None

experiments = [
    ("A_basic_aug",   ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True), "baseline", 1e-3),
    ("B_no_aug",      ImageDataGenerator(rescale=1./255), "baseline", 1e-3),
    ("C_dropout",     ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True), "dropout", 1e-3),
    ("D_batchnorm",   ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True), "batchnorm", 1e-3),
    ("E_lr_low",      ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True), "baseline", 1e-4)
]

for name, datagen, variant, lr in experiments:
    with mlflow.start_run(run_name=name) as run:
        mlflow.log_params({"variant": variant, "learning_rate": lr})
        train_gen, val_gen, test_gen = create_generators(datagen)
        model = build_model(variant, lr)

        hist = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, verbose=2)
        val_acc = max(hist.history["val_accuracy"])
        mlflow.log_metric("max_val_accuracy", val_acc)

        #  Zapisz model tymczasowy
        tmp_path = f"model_{name}.h5"
        model.save(tmp_path)
        mlflow.log_artifact(tmp_path)
        os.remove(tmp_path)
        #  najlepsza wersja modelu
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_run_id = run.info.run_id
            model.save("best_model.h5")

        # Oceń na zestawie testowym
        y_true = test_gen.classes
        y_pred_probs = model.predict(test_gen)
        y_pred = np.argmax(y_pred_probs, axis=1)

        report = classification_report(y_true, y_pred, output_dict=True)
        mlflow.log_metric("test_accuracy", report["accuracy"])

        cm = confusion_matrix(y_true, y_pred)

        # artefakt: matryca
        plt.figure(figsize=(6, 6))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=list(test_gen.class_indices), yticklabels=list(test_gen.class_indices))
        plt.title("Confusion Matrix")
        plt.tight_layout()
        plt.savefig("confusion_matrix.png")
        mlflow.log_artifact("confusion_matrix.png")
        os.remove("confusion_matrix.png")

        results[name] = {"val_acc": val_acc, "run_id": run.info.run_id}

# Walidacja na  VOLUMENACH
BRATS_TRAINING = {
    "flair":  os.path.join(DATASET_PATH, "BraTS20_Training_368_flair.nii"),
    "seg":    os.path.join(DATASET_PATH, "BraTS20_Training_368_seg.nii"),
    "t1":     os.path.join(DATASET_PATH, "BraTS20_Training_368_t1.nii"),
    "t1ce":   os.path.join(DATASET_PATH, "BraTS20_Training_368_t1ce.nii"),
    "t2":     os.path.join(DATASET_PATH, "BraTS20_Training_368_t2.nii")
}
BRATS_VALIDATION = {
    "flair":  os.path.join(DATASET_PATH, "BraTS20_Validation_124_flair.nii"),
    "t1":     os.path.join(DATASET_PATH, "BraTS20_Validation_124_t1.nii"),
    "t1ce":   os.path.join(DATASET_PATH, "BraTS20_Validation_124_t1ce.nii"),
    "t2":     os.path.join(DATASET_PATH, "BraTS20_Validation_124_t2.nii")
}

best_model = load_model("best_model.h5")
brats_results = []

for set_name, vol_dict in [("train", BRATS_TRAINING), ("val", BRATS_VALIDATION)]:
    img = preprocess_brats_volume(vol_dict, ["flair", "t1ce", "t2"])
    x = np.expand_dims(img, axis=0)
    preds = best_model.predict(x)[0]
    pred_class = int(np.argmax(preds))

    brats_results.append({
        "set_name": set_name,
        "predicted_class": pred_class,
        "probabilities": preds.tolist()
    })

    plt.imshow((img[..., 0] * 255).astype(np.uint8), cmap='gray')
    plt.title(f"{set_name.upper()} Predicted class: {pred_class}")
    plt.show()

with open("brats_preds.json", "w") as f:
    json.dump(brats_results, f, indent=2)
mlflow.log_artifact("brats_preds.json")
os.remove("brats_preds.json")
if os.path.exists("best_model.h5"):
    os.remove("best_model.h5")


DATASET_PATH = "/content/drive/MyDrive/dataset"
MODEL_SAVE_PATH = "/content/drive/MyDrive/best_model_mri.h5"
###############################################################
# Generatory danych
def create_generators():
    datagen = ImageDataGenerator(rescale=1./255)
    train = datagen.flow_from_directory(
        os.path.join(DATASET_PATH, "train"),
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical'
    )
    val = datagen.flow_from_directory(
        os.path.join(DATASET_PATH, "validation"),
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical'
    )
    test = datagen.flow_from_directory(
        os.path.join(DATASET_PATH, "test"),
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=False
    )
    return train, val, test

# Model CNN
def build_model():
    model = Sequential([
        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(128,128,3)),
        MaxPooling2D((2,2)),
        Conv2D(64, (3,3), activation='relu', padding='same'),
        MaxPooling2D((2,2)),
        Conv2D(128, (3,3), activation='relu', padding='same'),
        MaxPooling2D((2,2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Przygotowanie
train_gen, val_gen, test_gen = create_generators()
model = build_model()

# MLflow — bezpieczny start nowego runu
mlflow.set_experiment("MRI_Training")
active = mlflow.active_run()
start_args = dict(run_name="Final_Training_Run")
if active is not None:
    # jeżeli coś już jest aktywne (np. Azure/Colab autolog), to odpal z nested=True
    start_args["nested"] = True

with mlflow.start_run(**start_args):
    # Trening
    history = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, verbose=2)

    # Logi walidacyjne
    val_acc = float(np.max(history.history.get('val_accuracy', [0.0])))
    mlflow.log_metric("val_accuracy", val_acc)

    # Zapis modelu + log artefaktu
    model.save(MODEL_SAVE_PATH)
    print(f"Model zapisany do: {MODEL_SAVE_PATH}")
    mlflow.log_artifact(MODEL_SAVE_PATH)

    # Ewaluacja testowa
    y_true = test_gen.classes
    y_pred_probs = model.predict(test_gen, verbose=0)
    y_pred = np.argmax(y_pred_probs, axis=1)

    test_acc = float(accuracy_score(y_true, y_pred))
    mlflow.log_metric("test_accuracy", test_acc)

    # Raport tekstowy jako artefakt
    report_text = classification_report(y_true, y_pred)
    with open("classification_report.txt", "w") as f:
        f.write(report_text)
    mlflow.log_artifact("classification_report.txt")

    # Macierz pomyłek (z poprawnymi etykietami klas)
    idx_to_class = {v: k for k, v in test_gen.class_indices.items()}
    class_labels = [idx_to_class[i] for i in range(len(idx_to_class))]

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_labels, yticklabels=class_labels)
    plt.title("Macierz pomyłek")
    plt.xlabel("Przewidziane")
    plt.ylabel("Rzeczywiste")
    plt.tight_layout()
    plt.savefig("confusion_matrix.png")
    mlflow.log_artifact("confusion_matrix.png")
    plt.close()

#########################################################
# evaluate_model_with_histograms.py

MODEL_PATH = "/content/drive/MyDrive/best_model_mri.h5"
DATASET_PATH = "/content/drive/MyDrive/dataset"


# Generatory
datagen = ImageDataGenerator(rescale=1./255)
test_gen = datagen.flow_from_directory(
    os.path.join(DATASET_PATH, "test"),
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False
)

# Mapowanie klas
idx_to_class = {v: k for k, v in test_gen.class_indices.items()}
class_labels = [idx_to_class[i] for i in range(len(idx_to_class))]

# Wczytaj model
model = load_model(MODEL_PATH)

print("Model załadowany.")

# Predykcje
y_true = test_gen.classes
y_pred_probs = model.predict(test_gen, verbose=0)
y_pred = np.argmax(y_pred_probs, axis=1)

# Histogram błędnych predykcji
errors = (y_true != y_pred)
plt.figure(figsize=(8, 5))
sns.histplot(errors.astype(int), bins=2, discrete=True)
plt.xticks([0, 1], ["Poprawne", "Błędne"])
plt.title("Histogram błędnych predykcji")
plt.tight_layout()
plt.show()
plt.close()

# Histogram rozkładu klas przewidywanych
plt.figure(figsize=(8, 5))
sns.histplot(y_pred, bins=len(np.unique(y_true)), discrete=True)
plt.title("Rozkład przewidywanych klas")
plt.xlabel("Klasa (indeks)")
plt.ylabel("Liczba predykcji")
plt.tight_layout()
plt.show()
plt.close()

# ROC + AUC (one-vs-rest)
y_true_bin = label_binarize(y_true, classes=list(range(N_CLASSES)))
fpr, tpr, roc_auc = {}, {}, {}
for i in range(N_CLASSES):
    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(8, 6))
for i in range(N_CLASSES):
    plt.plot(fpr[i], tpr[i], label=f"{class_labels[i]} (AUC = {roc_auc[i]:.2f})")
plt.plot([0, 1], [0, 1], "k--", label="Losowa linia")
plt.title("ROC Curve dla każdej klasy")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.tight_layout()
plt.show()
plt.close()



